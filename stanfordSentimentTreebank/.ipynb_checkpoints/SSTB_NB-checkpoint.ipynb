{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>1</td>\n",
       "      <td>226166</td>\n",
       "      <td>0.69444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>226300</td>\n",
       "      <td>0.83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>1</td>\n",
       "      <td>225801</td>\n",
       "      <td>0.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>1</td>\n",
       "      <td>14646</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>1</td>\n",
       "      <td>14644</td>\n",
       "      <td>0.72222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8539</td>\n",
       "      <td>8539</td>\n",
       "      <td>11851</td>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>1</td>\n",
       "      <td>222071</td>\n",
       "      <td>0.11111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8540</td>\n",
       "      <td>8540</td>\n",
       "      <td>11852</td>\n",
       "      <td>No surprises .</td>\n",
       "      <td>1</td>\n",
       "      <td>225165</td>\n",
       "      <td>0.22222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8541</td>\n",
       "      <td>8541</td>\n",
       "      <td>11853</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>1</td>\n",
       "      <td>226985</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8542</td>\n",
       "      <td>8542</td>\n",
       "      <td>11854</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>223632</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8543</td>\n",
       "      <td>8543</td>\n",
       "      <td>11855</td>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>1</td>\n",
       "      <td>224044</td>\n",
       "      <td>0.34722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  sentence_index  \\\n",
       "0              0               1   \n",
       "1              1               2   \n",
       "2              2              61   \n",
       "3              3              62   \n",
       "4              4              63   \n",
       "...          ...             ...   \n",
       "8539        8539           11851   \n",
       "8540        8540           11852   \n",
       "8541        8541           11853   \n",
       "8542        8542           11854   \n",
       "8543        8543           11855   \n",
       "\n",
       "                                               sentence  splitset_label  \\\n",
       "0     The Rock is destined to be the 21st Century 's...               1   \n",
       "1     The gorgeously elaborate continuation of `` Th...               1   \n",
       "2     Singer\\/composer Bryan Adams contributes a sle...               1   \n",
       "3     You 'd think by now America would have had eno...               1   \n",
       "4                  Yet the act is still charming here .               1   \n",
       "...                                                 ...             ...   \n",
       "8539                                    A real snooze .               1   \n",
       "8540                                     No surprises .               1   \n",
       "8541  We 've seen the hippie-turned-yuppie plot befo...               1   \n",
       "8542  Her fans walked out muttering words like `` ho...               1   \n",
       "8543                                In this case zero .               1   \n",
       "\n",
       "          id   values  \n",
       "0     226166  0.69444  \n",
       "1     226300  0.83333  \n",
       "2     225801  0.62500  \n",
       "3      14646  0.50000  \n",
       "4      14644  0.72222  \n",
       "...      ...      ...  \n",
       "8539  222071  0.11111  \n",
       "8540  225165  0.22222  \n",
       "8541  226985  0.75000  \n",
       "8542  223632  0.13889  \n",
       "8543  224044  0.34722  \n",
       "\n",
       "[8544 rows x 6 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"SSTB_train.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>2</td>\n",
       "      <td>13995</td>\n",
       "      <td>0.513890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>2</td>\n",
       "      <td>14123</td>\n",
       "      <td>0.736110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>13999</td>\n",
       "      <td>0.861110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>14498</td>\n",
       "      <td>0.597220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>2</td>\n",
       "      <td>14351</td>\n",
       "      <td>0.833330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2205</td>\n",
       "      <td>2205</td>\n",
       "      <td>11621</td>\n",
       "      <td>An imaginative comedy\\/thriller .</td>\n",
       "      <td>2</td>\n",
       "      <td>13851</td>\n",
       "      <td>0.777780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2206</td>\n",
       "      <td>2206</td>\n",
       "      <td>11623</td>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "      <td>2</td>\n",
       "      <td>18182</td>\n",
       "      <td>0.916670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2207</td>\n",
       "      <td>2207</td>\n",
       "      <td>11626</td>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "      <td>2</td>\n",
       "      <td>23211</td>\n",
       "      <td>0.888890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2208</td>\n",
       "      <td>2208</td>\n",
       "      <td>11628</td>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "      <td>2</td>\n",
       "      <td>26177</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2209</td>\n",
       "      <td>2209</td>\n",
       "      <td>11798</td>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "      <td>2</td>\n",
       "      <td>141831</td>\n",
       "      <td>0.069444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2210 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  sentence_index  \\\n",
       "0              0               3   \n",
       "1              1               4   \n",
       "2              2               5   \n",
       "3              3               6   \n",
       "4              4               7   \n",
       "...          ...             ...   \n",
       "2205        2205           11621   \n",
       "2206        2206           11623   \n",
       "2207        2207           11626   \n",
       "2208        2208           11628   \n",
       "2209        2209           11798   \n",
       "\n",
       "                                               sentence  splitset_label  \\\n",
       "0                        Effective but too-tepid biopic               2   \n",
       "1     If you sometimes like to go to the movies to h...               2   \n",
       "2     Emerges as something rare , an issue movie tha...               2   \n",
       "3     The film provides some great insight into the ...               2   \n",
       "4     Offers that rare combination of entertainment ...               2   \n",
       "...                                                 ...             ...   \n",
       "2205                  An imaginative comedy\\/thriller .               2   \n",
       "2206                      ( A ) rare , beautiful film .               2   \n",
       "2207                 ( An ) hilarious romantic comedy .               2   \n",
       "2208                Never ( sinks ) into exploitation .               2   \n",
       "2209                        ( U ) nrelentingly stupid .               2   \n",
       "\n",
       "          id    values  \n",
       "0      13995  0.513890  \n",
       "1      14123  0.736110  \n",
       "2      13999  0.861110  \n",
       "3      14498  0.597220  \n",
       "4      14351  0.833330  \n",
       "...      ...       ...  \n",
       "2205   13851  0.777780  \n",
       "2206   18182  0.916670  \n",
       "2207   23211  0.888890  \n",
       "2208   26177  0.625000  \n",
       "2209  141831  0.069444  \n",
       "\n",
       "[2210 rows x 6 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"SSTB_test.csv\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting the y values to positive or negative values\n",
    "\n",
    "def convert_to_pos_neg(y):\n",
    "    for i in range(len(y)):\n",
    "        if y[i] >= 3:\n",
    "            y[i] = 1\n",
    "        elif y[i] <= 1:\n",
    "            y[i] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_fine_grain(y):\n",
    "    for i in range(len(y)):\n",
    "        if y[i] >= 0 and y[i] <= 0.2:\n",
    "            y[i] = 0\n",
    "        elif y[i] > 0.2 and y[i] < 0.4:\n",
    "            y[i] = 1\n",
    "        elif y[i] > 0.4 and y[i] < 0.6:\n",
    "            y[i] = 2\n",
    "        elif y[i] > 0.6 and y[i] < 0.8:\n",
    "            y[i] = 3\n",
    "        elif y[i] > 0.8 and y[i] <= 1.0:\n",
    "            y[i] = 4\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset labels into fine grain classification\n",
    "df_train[\"values\"] = convert_to_fine_grain(df_train[\"values\"])\n",
    "df_test[\"values\"] = convert_to_fine_grain(df_test[\"values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into positive / negative binary classification\n",
    "# In order to not to alter the original dataset, we make a copy of each\n",
    "df_train_copy = df_train.copy()\n",
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>1</td>\n",
       "      <td>226166</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>226300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>1</td>\n",
       "      <td>225801</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>1</td>\n",
       "      <td>14646</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>1</td>\n",
       "      <td>14644</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8539</td>\n",
       "      <td>8539</td>\n",
       "      <td>11851</td>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>1</td>\n",
       "      <td>222071</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8540</td>\n",
       "      <td>8540</td>\n",
       "      <td>11852</td>\n",
       "      <td>No surprises .</td>\n",
       "      <td>1</td>\n",
       "      <td>225165</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8541</td>\n",
       "      <td>8541</td>\n",
       "      <td>11853</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>1</td>\n",
       "      <td>226985</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8542</td>\n",
       "      <td>8542</td>\n",
       "      <td>11854</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>223632</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8543</td>\n",
       "      <td>8543</td>\n",
       "      <td>11855</td>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>1</td>\n",
       "      <td>224044</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  sentence_index  \\\n",
       "0              0               1   \n",
       "1              1               2   \n",
       "2              2              61   \n",
       "3              3              62   \n",
       "4              4              63   \n",
       "...          ...             ...   \n",
       "8539        8539           11851   \n",
       "8540        8540           11852   \n",
       "8541        8541           11853   \n",
       "8542        8542           11854   \n",
       "8543        8543           11855   \n",
       "\n",
       "                                               sentence  splitset_label  \\\n",
       "0     The Rock is destined to be the 21st Century 's...               1   \n",
       "1     The gorgeously elaborate continuation of `` Th...               1   \n",
       "2     Singer\\/composer Bryan Adams contributes a sle...               1   \n",
       "3     You 'd think by now America would have had eno...               1   \n",
       "4                  Yet the act is still charming here .               1   \n",
       "...                                                 ...             ...   \n",
       "8539                                    A real snooze .               1   \n",
       "8540                                     No surprises .               1   \n",
       "8541  We 've seen the hippie-turned-yuppie plot befo...               1   \n",
       "8542  Her fans walked out muttering words like `` ho...               1   \n",
       "8543                                In this case zero .               1   \n",
       "\n",
       "          id  values  \n",
       "0     226166     1.0  \n",
       "1     226300     1.0  \n",
       "2     225801     1.0  \n",
       "3      14646     2.0  \n",
       "4      14644     1.0  \n",
       "...      ...     ...  \n",
       "8539  222071     0.0  \n",
       "8540  225165     0.0  \n",
       "8541  226985     1.0  \n",
       "8542  223632     0.0  \n",
       "8543  224044     0.0  \n",
       "\n",
       "[8544 rows x 6 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Firstly, convert the target values into positive / negative, except for neutral ones\n",
    "# which will be removed after\n",
    "df_train_copy[\"values\"] = convert_to_pos_neg(df_train_copy[\"values\"])\n",
    "df_test_copy[\"values\"] = convert_to_pos_neg(df_test_copy[\"values\"])\n",
    "df_train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>2</td>\n",
       "      <td>14123</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>13999</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>2</td>\n",
       "      <td>14351</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>Perhaps no picture ever made has more literall...</td>\n",
       "      <td>2</td>\n",
       "      <td>14371</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Steers turns in a snappy screenplay that curls...</td>\n",
       "      <td>2</td>\n",
       "      <td>225968</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2205</td>\n",
       "      <td>2205</td>\n",
       "      <td>11621</td>\n",
       "      <td>An imaginative comedy\\/thriller .</td>\n",
       "      <td>2</td>\n",
       "      <td>13851</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2206</td>\n",
       "      <td>2206</td>\n",
       "      <td>11623</td>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "      <td>2</td>\n",
       "      <td>18182</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2207</td>\n",
       "      <td>2207</td>\n",
       "      <td>11626</td>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "      <td>2</td>\n",
       "      <td>23211</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2208</td>\n",
       "      <td>2208</td>\n",
       "      <td>11628</td>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "      <td>2</td>\n",
       "      <td>26177</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2209</td>\n",
       "      <td>2209</td>\n",
       "      <td>11798</td>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "      <td>2</td>\n",
       "      <td>141831</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  sentence_index  \\\n",
       "1              1               4   \n",
       "2              2               5   \n",
       "4              4               7   \n",
       "5              5               8   \n",
       "6              6               9   \n",
       "...          ...             ...   \n",
       "2205        2205           11621   \n",
       "2206        2206           11623   \n",
       "2207        2207           11626   \n",
       "2208        2208           11628   \n",
       "2209        2209           11798   \n",
       "\n",
       "                                               sentence  splitset_label  \\\n",
       "1     If you sometimes like to go to the movies to h...               2   \n",
       "2     Emerges as something rare , an issue movie tha...               2   \n",
       "4     Offers that rare combination of entertainment ...               2   \n",
       "5     Perhaps no picture ever made has more literall...               2   \n",
       "6     Steers turns in a snappy screenplay that curls...               2   \n",
       "...                                                 ...             ...   \n",
       "2205                  An imaginative comedy\\/thriller .               2   \n",
       "2206                      ( A ) rare , beautiful film .               2   \n",
       "2207                 ( An ) hilarious romantic comedy .               2   \n",
       "2208                Never ( sinks ) into exploitation .               2   \n",
       "2209                        ( U ) nrelentingly stupid .               2   \n",
       "\n",
       "          id  values  \n",
       "1      14123     1.0  \n",
       "2      13999     1.0  \n",
       "4      14351     1.0  \n",
       "5      14371     1.0  \n",
       "6     225968     1.0  \n",
       "...      ...     ...  \n",
       "2205   13851     1.0  \n",
       "2206   18182     1.0  \n",
       "2207   23211     1.0  \n",
       "2208   26177     1.0  \n",
       "2209  141831     0.0  \n",
       "\n",
       "[1821 rows x 6 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the neutral values for positive / negative classification\n",
    "df_train_copy.drop(df_train_copy[df_train_copy[\"values\"] == 2].index, inplace=True)\n",
    "df_test_copy.drop(df_test_copy[df_test_copy[\"values\"] == 2].index, inplace=True)\n",
    "df_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_posneg = df_train_copy[\"sentence\"]\n",
    "y_train_posneg = df_train_copy[\"values\"]\n",
    "\n",
    "X_test_posneg = df_test_copy[\"sentence\"]\n",
    "y_test_posneg = df_test_copy[\"values\"]\n",
    "\n",
    "X_train_finegrain = df_train[\"sentence\"]\n",
    "y_train_finegrain = df_train[\"values\"]\n",
    "\n",
    "X_test_finegrain = df_test[\"sentence\"]\n",
    "y_test_finegrain = df_test[\"values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_posneg_cvec = cvec.fit_transform(X_train_posneg, y_train_posneg)\n",
    "X_train_posneg_tf = tfidf.fit_transform(X_train_posneg, y_train_posneg)\n",
    "\n",
    "X_test_posneg_cvec = cvec.transform(X_test_posneg)\n",
    "X_test_posneg_tf = tfidf.transform(X_test_posneg)\n",
    "\n",
    "X_train_finegrain_cvec = cvec.fit_transform(X_train_finegrain, y_train_finegrain)\n",
    "X_train_finegrain_tf = tfidf.fit_transform(X_train_finegrain, y_train_finegrain)\n",
    "\n",
    "X_test_finegrain_cvec = cvec.transform(X_test_finegrain)\n",
    "X_test_finegrain_tf = tfidf.transform(X_test_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation with countvectorizer:  0.7927745664739884\n",
      "Cross validation with TfidfVectorizer:  0.7882947976878614\n",
      "Time Execution: 0.08514980800100602\n"
     ]
    }
   ],
   "source": [
    "#=====================Generic 5-fold Cross Validation Score On Classifying Positive/Negative==================\n",
    "# This cross validation will only operate on the training set\n",
    "# Start the Timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "results_cv = model_selection.cross_val_score(mnb, X_train_posneg_cvec, y_train_posneg, cv = 5)\n",
    "print(\"Cross validation with countvectorizer: \", results_cv.mean())\n",
    "\n",
    "results_tf = model_selection.cross_val_score(mnb, X_train_posneg_tf, y_train_posneg, cv = 5)\n",
    "print(\"Cross validation with TfidfVectorizer: \", results_tf.mean())\n",
    "# mnb.fit(X_train_tf, y_train)\n",
    "# y_pred = mnb.predict(X_test_tf)\n",
    "\n",
    "# print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "# Timer stops\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time Execution: {}\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation with countvectorizer:  0.3959462336233364\n",
      "Cross validation with TfidfVectorizer:  0.39465968487991576\n",
      "Time Execution: 0.12543005600309698\n"
     ]
    }
   ],
   "source": [
    "#=====================Generic 5-fold Cross Validation Score on Classifying Fine Grain==================\n",
    "# This cross validation will only operate on the training set\n",
    "# Start the Timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "results_cv = model_selection.cross_val_score(mnb, X_train_finegrain_cvec, y_train_finegrain, cv = 5)\n",
    "print(\"Cross validation with countvectorizer: \", results_cv.mean())\n",
    "\n",
    "results_tf = model_selection.cross_val_score(mnb, X_train_finegrain_tf, y_train_finegrain, cv = 5)\n",
    "print(\"Cross validation with TfidfVectorizer: \", results_tf.mean())\n",
    "# mnb.fit(X_train_tf, y_train)\n",
    "# y_pred = mnb.predict(X_test_tf)\n",
    "\n",
    "# print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "# Timer stops\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time Execution: {}\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Positive/Negative Classification========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================Finding Optimal N_gram============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the accuracy of a given classifier and vectorizer\n",
    "def nfeature_accuracy_checker(X_train, y_train, X_test, y_test, vectorizer=None, n_features=None, stop_words=None, ngram_range=(1, 1), classifier=None):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        t0 = time()\n",
    "        sentiment_fit = checker_pipeline.fit(X_train, y_train)\n",
    "        y_pred = sentiment_fit.predict(X_test)\n",
    "        train_test_time = time() - t0\n",
    "        accuracy = accuracy_score(y_pred, y_test)\n",
    "        print(\"accuracy with\", n, \"features:\", accuracy)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_extractor = [None, \"english\"]\n",
    "vectorizer = [cvec, tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 10000 features: 0.8127402526084568\n",
      "accuracy with 20000 features: 0.814936847885777\n",
      "accuracy with 30000 features: 0.814936847885777\n",
      "accuracy with 40000 features: 0.814936847885777\n",
      "accuracy with 50000 features: 0.814936847885777\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 10000 features: 0.8094453596924767\n",
      "accuracy with 20000 features: 0.8121911037891268\n",
      "accuracy with 30000 features: 0.8121911037891268\n",
      "accuracy with 40000 features: 0.8121911037891268\n",
      "accuracy with 50000 features: 0.8121911037891268\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 10000 features: 0.8012081274025261\n",
      "accuracy with 20000 features: 0.8094453596924767\n",
      "accuracy with 30000 features: 0.8094453596924767\n",
      "accuracy with 40000 features: 0.8094453596924767\n",
      "accuracy with 50000 features: 0.8094453596924767\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 10000 features: 0.8017572762218561\n",
      "accuracy with 20000 features: 0.7979132344865458\n",
      "accuracy with 30000 features: 0.7979132344865458\n",
      "accuracy with 40000 features: 0.7979132344865458\n",
      "accuracy with 50000 features: 0.7979132344865458\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_features = np.arange(10000,50001,10000)\n",
    "\n",
    "# unigram case \n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_posneg, y_train_posneg, X_test_posneg, y_test_posneg,\n",
    "                                                                                   vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 30000 features: 0.8171334431630972\n",
      "accuracy with 40000 features: 0.8176825919824272\n",
      "accuracy with 50000 features: 0.8138385502471169\n",
      "accuracy with 60000 features: 0.8198791872597474\n",
      "accuracy with 70000 features: 0.8204283360790774\n",
      "accuracy with 80000 features: 0.8215266337177375\n",
      "accuracy with 90000 features: 0.8215266337177375\n",
      "accuracy with 100000 features: 0.8215266337177375\n",
      "accuracy with 110000 features: 0.8215266337177375\n",
      "accuracy with 120000 features: 0.8215266337177375\n",
      "accuracy with 130000 features: 0.8215266337177375\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 30000 features: 0.8132894014277869\n",
      "accuracy with 40000 features: 0.8127402526084568\n",
      "accuracy with 50000 features: 0.8116419549697969\n",
      "accuracy with 60000 features: 0.8132894014277869\n",
      "accuracy with 70000 features: 0.8077979132344866\n",
      "accuracy with 80000 features: 0.8056013179571664\n",
      "accuracy with 90000 features: 0.8056013179571664\n",
      "accuracy with 100000 features: 0.8056013179571664\n",
      "accuracy with 110000 features: 0.8056013179571664\n",
      "accuracy with 120000 features: 0.8056013179571664\n",
      "accuracy with 130000 features: 0.8056013179571664\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 30000 features: 0.7973640856672158\n",
      "accuracy with 40000 features: 0.7995606809445359\n",
      "accuracy with 50000 features: 0.7979132344865458\n",
      "accuracy with 60000 features: 0.7973640856672158\n",
      "accuracy with 70000 features: 0.7995606809445359\n",
      "accuracy with 80000 features: 0.7995606809445359\n",
      "accuracy with 90000 features: 0.7995606809445359\n",
      "accuracy with 100000 features: 0.7995606809445359\n",
      "accuracy with 110000 features: 0.7995606809445359\n",
      "accuracy with 120000 features: 0.7995606809445359\n",
      "accuracy with 130000 features: 0.7995606809445359\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 30000 features: 0.7957166392092258\n",
      "accuracy with 40000 features: 0.7990115321252059\n",
      "accuracy with 50000 features: 0.8023064250411862\n",
      "accuracy with 60000 features: 0.7968149368478857\n",
      "accuracy with 70000 features: 0.7957166392092258\n",
      "accuracy with 80000 features: 0.7957166392092258\n",
      "accuracy with 90000 features: 0.7957166392092258\n",
      "accuracy with 100000 features: 0.7957166392092258\n",
      "accuracy with 110000 features: 0.7957166392092258\n",
      "accuracy with 120000 features: 0.7957166392092258\n",
      "accuracy with 130000 features: 0.7957166392092258\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram case \n",
    "# in here, we modify the number of features from the range 30000 to 130000, since \n",
    "# much more features will be created\n",
    "\n",
    "n_features = np.arange(30000,130001,10000)\n",
    "\n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_posneg, y_train_posneg, X_test_posneg, y_test_posneg,\n",
    "                                                                vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], ngram_range = (1, 2), classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 60000 features: 0.8116419549697969\n",
      "accuracy with 70000 features: 0.8138385502471169\n",
      "accuracy with 80000 features: 0.8132894014277869\n",
      "accuracy with 90000 features: 0.8154859967051071\n",
      "accuracy with 100000 features: 0.8176825919824272\n",
      "accuracy with 110000 features: 0.8171334431630972\n",
      "accuracy with 120000 features: 0.8193300384404174\n",
      "accuracy with 130000 features: 0.8198791872597474\n",
      "accuracy with 140000 features: 0.8187808896210873\n",
      "accuracy with 150000 features: 0.814387699066447\n",
      "accuracy with 160000 features: 0.814936847885777\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 60000 features: 0.8072487644151565\n",
      "accuracy with 70000 features: 0.8061504667764964\n",
      "accuracy with 80000 features: 0.8023064250411862\n",
      "accuracy with 90000 features: 0.8050521691378364\n",
      "accuracy with 100000 features: 0.8056013179571664\n",
      "accuracy with 110000 features: 0.8066996155958265\n",
      "accuracy with 120000 features: 0.8066996155958265\n",
      "accuracy with 130000 features: 0.8045030203185063\n",
      "accuracy with 140000 features: 0.8023064250411862\n",
      "accuracy with 150000 features: 0.800109829763866\n",
      "accuracy with 160000 features: 0.7995606809445359\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 60000 features: 0.7951674903898956\n",
      "accuracy with 70000 features: 0.7984623833058759\n",
      "accuracy with 80000 features: 0.800658978583196\n",
      "accuracy with 90000 features: 0.800658978583196\n",
      "accuracy with 100000 features: 0.7995606809445359\n",
      "accuracy with 110000 features: 0.7962657880285557\n",
      "accuracy with 120000 features: 0.7962657880285557\n",
      "accuracy with 130000 features: 0.7962657880285557\n",
      "accuracy with 140000 features: 0.7962657880285557\n",
      "accuracy with 150000 features: 0.7962657880285557\n",
      "accuracy with 160000 features: 0.7962657880285557\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 60000 features: 0.7913234486545854\n",
      "accuracy with 70000 features: 0.7924217462932455\n",
      "accuracy with 80000 features: 0.7968149368478857\n",
      "accuracy with 90000 features: 0.7962657880285557\n",
      "accuracy with 100000 features: 0.7951674903898956\n",
      "accuracy with 110000 features: 0.7940691927512356\n",
      "accuracy with 120000 features: 0.7940691927512356\n",
      "accuracy with 130000 features: 0.7940691927512356\n",
      "accuracy with 140000 features: 0.7940691927512356\n",
      "accuracy with 150000 features: 0.7940691927512356\n",
      "accuracy with 160000 features: 0.7940691927512356\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram + trigram case\n",
    "# We furthur extent the number of features to the range of 120000\n",
    "\n",
    "n_features = np.arange(60000,160001,10000)\n",
    "\n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_posneg, y_train_posneg, X_test_posneg, y_test_posneg,\n",
    "                                                                vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], ngram_range = (1, 3), classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, the bigram case with CountVectorizer 80000 features has the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnbGV = MultinomialNB()\n",
    "\n",
    "\n",
    "cvecGV = CountVectorizer(ngram_range = (1, 2), max_features = 80000)\n",
    "X_cvec = cvecGV.fit_transform(X_train_posneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.75632022 0.75784176 0.75772472 0.75760768 0.75632022]\n",
      "scores_std [0.03872693 0.03740403 0.03728401 0.03769677 0.03742853]\n",
      "{'alpha': 1.7}\n",
      "Time Execution: 0.3508574940001381\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------GRID SEARCH CROSS VALIDATION------------------------------\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Timer begins\n",
    "start = timeit.default_timer()\n",
    "\n",
    "tuned_parameters = [{'alpha' : [1.5, 1.7, 1.75, 1.8, 2.0]}]\n",
    "n_folds = 5\n",
    "\n",
    "grid_search = GridSearchCV(estimator = mnb, param_grid = tuned_parameters, cv = n_folds, refit = False, n_jobs = -1)\n",
    "\n",
    "grid_search.fit(X_cvec, y_train_posneg)\n",
    "\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "scores_std = grid_search.cv_results_['std_test_score']\n",
    "print('scores:',scores)\n",
    "print('scores_std',scores_std)\n",
    "\n",
    "# Optimal hyperparameter\n",
    "bestAlpha = grid_search.best_params_['alpha']\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Timer stops\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time Execution: {}\".format(stop - start))\n",
    "# #-----------------------------END OF GRID SEARCH-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning(alphas):\n",
    "    best_alpha = None\n",
    "    best_result = 0\n",
    "    \n",
    "    for a in alphas:\n",
    "        mnb_model = MultinomialNB(alpha = a)\n",
    "        X_final = cvecGV.transform(X_test_posneg)\n",
    "        mnb_model.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = mnb_model.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"accuracy for alpha =\", a, \":\", result)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_alpha = a\n",
    "        \n",
    "    print(\"The best result is\",best_result,\"with alpha of\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for alpha = 0.7 : 0.8193300384404174\n",
      "accuracy for alpha = 0.8 : 0.8187808896210873\n",
      "accuracy for alpha = 0.9 : 0.8215266337177375\n",
      "accuracy for alpha = 1.0 : 0.8215266337177375\n",
      "accuracy for alpha = 1.1 : 0.8198791872597474\n",
      "accuracy for alpha = 1.2 : 0.8193300384404174\n",
      "The best result is 0.8215266337177375 with alpha of 0.9\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.7, 0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "hyper_parameter_tuning(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparameter value so far is 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================Bagging Classifier with NB Model================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Timer begins\n",
    "# start = timeit.default_timer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 0.9)\n",
    "X_final = cvecGV.transform(X_test_posneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_sample_optimization(samples):\n",
    "    best_sample = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for s in samples:\n",
    "        bg = BaggingClassifier(mnb, max_samples = s, max_features = 0.5, n_estimators = 200)\n",
    "        bg.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {}\".format(s, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_sample = s\n",
    "    print(\"The best result is {} ratio of sample with accuracy of {}\".format(best_sample, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.5 ratio of sample is 0.8088962108731467\n",
      "Bagging Classifier with 0.6 ratio of sample is 0.8083470620538166\n",
      "Bagging Classifier with 0.7 ratio of sample is 0.8121911037891268\n",
      "Bagging Classifier with 0.8 ratio of sample is 0.8105436573311368\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.8154859967051071\n",
      "Bagging Classifier with 1.0 ratio of sample is 0.8209774848984075\n",
      "The best result is 1.0 ratio of sample with accuracy of 0.8209774848984075\n"
     ]
    }
   ],
   "source": [
    "samples = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "bagging_max_sample_optimization(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best ratio is so far is 0.9, so we set the best sample ratio as a variable\n",
    "bestSampleRate = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_feature_optimization(features):\n",
    "    best_feature = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for f in features:\n",
    "        bg = BaggingClassifier(mnb, max_samples = bestSampleRate, max_features = f, n_estimators = 200)\n",
    "        bg.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {}\".format(f, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_feature = f\n",
    "    print(\"The best result is {} ratio of sample with accuracy of {}\".format(best_feature, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.4 ratio of sample is 0.8160351455244371\n",
      "Bagging Classifier with 0.5 ratio of sample is 0.8242723778143877\n",
      "Bagging Classifier with 0.6 ratio of sample is 0.8231740801757276\n",
      "Bagging Classifier with 0.7 ratio of sample is 0.8226249313563976\n",
      "Bagging Classifier with 0.8 ratio of sample is 0.8187808896210873\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.8215266337177375\n",
      "The best result is 0.5 ratio of sample with accuracy of 0.8242723778143877\n"
     ]
    }
   ],
   "source": [
    "features = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "bagging_max_feature_optimization(features)\n",
    "\n",
    "# # Timer stops\n",
    "# stop = timeit.default_timer()\n",
    "# print(\"Time Execution bagging classifier: {}\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatureRatio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_estimator_optimization(estimators):\n",
    "    best_estim = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for e in estimators:\n",
    "        bg = BaggingClassifier(mnb, max_samples = bestSampleRate, max_features = bestFeatureRatio, n_estimators = e)\n",
    "        bg.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {}\".format(e, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_estim = e\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_estim, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 400 ratio of sample is 0.8154859967051071\n",
      "Bagging Classifier with 600 ratio of sample is 0.8215266337177375\n",
      "Bagging Classifier with 800 ratio of sample is 0.8171334431630972\n",
      "Bagging Classifier with 1000 ratio of sample is 0.8187808896210873\n",
      "The best result is 600 estimators with accuracy of 0.8215266337177375\n"
     ]
    }
   ],
   "source": [
    "estimators = [400, 600, 800, 1000]\n",
    "bagging_max_estimator_optimization(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== AdaBoost Optimization===================\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 0.9)\n",
    "X_final = cvecGV.transform(X_test_posneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_max_estimator_optimization(estimators):\n",
    "    best_estim = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for e in estimators:\n",
    "        adb = AdaBoostClassifier(mnb, n_estimators = e, learning_rate = 0.5)\n",
    "        adb.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = adb.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {}\".format(e, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_estim = e\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_estim, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 400 ratio of sample is 0.7770455793520044\n",
      "Bagging Classifier with 600 ratio of sample is 0.7874794069192751\n",
      "Bagging Classifier with 800 ratio of sample is 0.786381109280615\n",
      "Bagging Classifier with 1000 ratio of sample is 0.786381109280615\n",
      "The best result is 600 estimators with accuracy of 0.7874794069192751\n"
     ]
    }
   ],
   "source": [
    "estimators = [400, 600, 800, 1000]\n",
    "boosting_max_estimator_optimization(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestEstimators = 600\n",
    "\n",
    "def boosting_max_estimator_optimization(learningrates):\n",
    "    best_lr = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for lr in learningrates:\n",
    "        adb = AdaBoostClassifier(mnb, n_estimators = bestEstimators, learning_rate = lr)\n",
    "        adb.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = adb.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {}\".format(lr, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_lr= lr\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_lr, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.8 ratio of sample is 0.786381109280615\n",
      "Bagging Classifier with 1 ratio of sample is 0.786381109280615\n",
      "Bagging Classifier with 1.1 ratio of sample is 0.7935200439319056\n",
      "Bagging Classifier with 1.2 ratio of sample is 0.800109829763866\n",
      "Bagging Classifier with 1.3 ratio of sample is 0.7891268533772653\n",
      "The best result is 1.2 estimators with accuracy of 0.800109829763866\n"
     ]
    }
   ],
   "source": [
    "learningrates = [0.8, 1, 1.1, 1.2, 1.3]\n",
    "boosting_max_estimator_optimization(learningrates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================End of Positive / Negative Section=================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================Fine Grain Section==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_extractor = [None, \"english\"]\n",
    "vectorizer = [cvec, tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.41266968325791853\n",
      "accuracy with 5000 features: 0.4171945701357466\n",
      "accuracy with 7500 features: 0.4108597285067873\n",
      "accuracy with 10000 features: 0.4072398190045249\n",
      "accuracy with 12500 features: 0.4076923076923077\n",
      "accuracy with 15000 features: 0.4113122171945701\n",
      "accuracy with 17500 features: 0.4099547511312217\n",
      "accuracy with 20000 features: 0.4099547511312217\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.39819004524886875\n",
      "accuracy with 5000 features: 0.40361990950226245\n",
      "accuracy with 7500 features: 0.40180995475113124\n",
      "accuracy with 10000 features: 0.3995475113122172\n",
      "accuracy with 12500 features: 0.3990950226244344\n",
      "accuracy with 15000 features: 0.39728506787330314\n",
      "accuracy with 17500 features: 0.39728506787330314\n",
      "accuracy with 20000 features: 0.39728506787330314\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.38371040723981903\n",
      "accuracy with 5000 features: 0.39592760180995473\n",
      "accuracy with 7500 features: 0.3945701357466063\n",
      "accuracy with 10000 features: 0.39638009049773754\n",
      "accuracy with 12500 features: 0.3950226244343891\n",
      "accuracy with 15000 features: 0.3936651583710407\n",
      "accuracy with 17500 features: 0.3936651583710407\n",
      "accuracy with 20000 features: 0.3936651583710407\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.38416289592760183\n",
      "accuracy with 5000 features: 0.3950226244343891\n",
      "accuracy with 7500 features: 0.3950226244343891\n",
      "accuracy with 10000 features: 0.3918552036199095\n",
      "accuracy with 12500 features: 0.3941176470588235\n",
      "accuracy with 15000 features: 0.3932126696832579\n",
      "accuracy with 17500 features: 0.3932126696832579\n",
      "accuracy with 20000 features: 0.3932126696832579\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_features = np.arange(2500,20001,2500)\n",
    "\n",
    "# unigram case \n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_finegrain, y_train_finegrain, X_test_finegrain, y_test_finegrain,\n",
    "                                                                                   vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.38823529411764707\n",
      "accuracy with 5000 features: 0.39728506787330314\n",
      "accuracy with 7500 features: 0.41266968325791853\n",
      "accuracy with 10000 features: 0.4085972850678733\n",
      "accuracy with 12500 features: 0.4095022624434389\n",
      "accuracy with 15000 features: 0.4072398190045249\n",
      "accuracy with 17500 features: 0.40407239819004526\n",
      "accuracy with 20000 features: 0.4072398190045249\n",
      "accuracy with 22500 features: 0.4095022624434389\n",
      "accuracy with 25000 features: 0.4117647058823529\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.3832579185520362\n",
      "accuracy with 5000 features: 0.38868778280542987\n",
      "accuracy with 7500 features: 0.3904977375565611\n",
      "accuracy with 10000 features: 0.3891402714932127\n",
      "accuracy with 12500 features: 0.3914027149321267\n",
      "accuracy with 15000 features: 0.38868778280542987\n",
      "accuracy with 17500 features: 0.38823529411764707\n",
      "accuracy with 20000 features: 0.3909502262443439\n",
      "accuracy with 22500 features: 0.3909502262443439\n",
      "accuracy with 25000 features: 0.3895927601809955\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.3895927601809955\n",
      "accuracy with 5000 features: 0.3891402714932127\n",
      "accuracy with 7500 features: 0.3909502262443439\n",
      "accuracy with 10000 features: 0.3950226244343891\n",
      "accuracy with 12500 features: 0.3941176470588235\n",
      "accuracy with 15000 features: 0.3923076923076923\n",
      "accuracy with 17500 features: 0.3927601809954751\n",
      "accuracy with 20000 features: 0.3914027149321267\n",
      "accuracy with 22500 features: 0.38778280542986426\n",
      "accuracy with 25000 features: 0.38642533936651585\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.3891402714932127\n",
      "accuracy with 5000 features: 0.38642533936651585\n",
      "accuracy with 7500 features: 0.3895927601809955\n",
      "accuracy with 10000 features: 0.38642533936651585\n",
      "accuracy with 12500 features: 0.38371040723981903\n",
      "accuracy with 15000 features: 0.38280542986425337\n",
      "accuracy with 17500 features: 0.38280542986425337\n",
      "accuracy with 20000 features: 0.38054298642533935\n",
      "accuracy with 22500 features: 0.38235294117647056\n",
      "accuracy with 25000 features: 0.38009049773755654\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram case \n",
    "# in here, we modify the number of features from the range 30000 to 130000, since \n",
    "# much more features will be created\n",
    "\n",
    "n_features = np.arange(2500,25001,2500)\n",
    "\n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_finegrain, y_train_finegrain, X_test_finegrain, y_test_finegrain,\n",
    "                                                                vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], ngram_range = (1, 2), classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 20000 features: 0.40180995475113124\n",
      "accuracy with 25000 features: 0.4108597285067873\n",
      "accuracy with 30000 features: 0.41312217194570133\n",
      "accuracy with 35000 features: 0.4063348416289593\n",
      "accuracy with 40000 features: 0.40361990950226245\n",
      "accuracy with 45000 features: 0.3986425339366516\n",
      "accuracy with 50000 features: 0.40135746606334843\n",
      "accuracy with 55000 features: 0.3995475113122172\n",
      "accuracy with 60000 features: 0.4\n",
      "accuracy with 65000 features: 0.4004524886877828\n",
      "accuracy with 70000 features: 0.3995475113122172\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 20000 features: 0.3904977375565611\n",
      "accuracy with 25000 features: 0.38868778280542987\n",
      "accuracy with 30000 features: 0.3891402714932127\n",
      "accuracy with 35000 features: 0.3914027149321267\n",
      "accuracy with 40000 features: 0.3923076923076923\n",
      "accuracy with 45000 features: 0.3909502262443439\n",
      "accuracy with 50000 features: 0.3945701357466063\n",
      "accuracy with 55000 features: 0.3918552036199095\n",
      "accuracy with 60000 features: 0.3904977375565611\n",
      "accuracy with 65000 features: 0.3900452488687783\n",
      "accuracy with 70000 features: 0.3895927601809955\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 20000 features: 0.3932126696832579\n",
      "accuracy with 25000 features: 0.38733031674208146\n",
      "accuracy with 30000 features: 0.38461538461538464\n",
      "accuracy with 35000 features: 0.38642533936651585\n",
      "accuracy with 40000 features: 0.38687782805429866\n",
      "accuracy with 45000 features: 0.38823529411764707\n",
      "accuracy with 50000 features: 0.38778280542986426\n",
      "accuracy with 55000 features: 0.3891402714932127\n",
      "accuracy with 60000 features: 0.3895927601809955\n",
      "accuracy with 65000 features: 0.3895927601809955\n",
      "accuracy with 70000 features: 0.38687782805429866\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 20000 features: 0.38144796380090495\n",
      "accuracy with 25000 features: 0.37963800904977374\n",
      "accuracy with 30000 features: 0.3778280542986425\n",
      "accuracy with 35000 features: 0.3773755656108597\n",
      "accuracy with 40000 features: 0.37918552036199094\n",
      "accuracy with 45000 features: 0.3782805429864253\n",
      "accuracy with 50000 features: 0.3769230769230769\n",
      "accuracy with 55000 features: 0.3769230769230769\n",
      "accuracy with 60000 features: 0.37918552036199094\n",
      "accuracy with 65000 features: 0.37918552036199094\n",
      "accuracy with 70000 features: 0.3769230769230769\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram + trigram case\n",
    "# We furthur extent the number of features to the range of 120000\n",
    "\n",
    "n_features = np.arange(20000,70001,5000)\n",
    "\n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_finegrain, y_train_finegrain, X_test_finegrain, y_test_finegrain,\n",
    "                                                                vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], ngram_range = (1, 3), classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, the unigram case with CountVectorizer 5000 features has the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnbGV = MultinomialNB()\n",
    "\n",
    "\n",
    "cvecGV = CountVectorizer(ngram_range = (1, 1), max_features = 5000)\n",
    "X_cvec = cvecGV.fit_transform(X_train_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_fg(alphas):\n",
    "    best_alpha = None\n",
    "    best_result = 0\n",
    "    \n",
    "    for a in alphas:\n",
    "        mnb_model = MultinomialNB(alpha = a)\n",
    "        X_final = cvecGV.transform(X_test)\n",
    "        mnb_model.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = mnb_model.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"accuracy for alpha =\", a, \":\", result)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_alpha = a\n",
    "        \n",
    "    print(\"The best result is\",best_result,\"with alpha of\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for alpha = 0.2 : 0.4108597285067873\n",
      "accuracy for alpha = 0.3 : 0.416289592760181\n",
      "accuracy for alpha = 0.4 : 0.41990950226244345\n",
      "accuracy for alpha = 0.5 : 0.42036199095022625\n",
      "accuracy for alpha = 0.6 : 0.42036199095022625\n",
      "The best result is 0.42036199095022625 with alpha of 0.5\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "hyper_parameter_tuning_fg(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, the best alpha is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 0.5)\n",
    "X_final = cvecGV.transform(X_test_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_sample_optimization_fg(samples):\n",
    "    best_sample = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for s in samples:\n",
    "        bg = BaggingClassifier(mnb, max_samples = s, max_features = 0.5, n_estimators = 200)\n",
    "        bg.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {}\".format(s, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_sample = s\n",
    "    print(\"The best result is {} ratio of sample with accuracy of {}\".format(best_sample, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.4 ratio of sample is 0.40452488687782806\n",
      "Bagging Classifier with 0.5 ratio of sample is 0.4095022624434389\n",
      "Bagging Classifier with 0.6 ratio of sample is 0.4167420814479638\n",
      "Bagging Classifier with 0.7 ratio of sample is 0.41945701357466064\n",
      "Bagging Classifier with 0.8 ratio of sample is 0.4117647058823529\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.41855203619909503\n",
      "Bagging Classifier with 1.0 ratio of sample is 0.4076923076923077\n",
      "The best result is 0.7 ratio of sample with accuracy of 0.41945701357466064\n"
     ]
    }
   ],
   "source": [
    "features = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "bagging_max_sample_optimization_fg(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best sample ratio was 0.7\n",
    "bestSampleRatioFG = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_feature_optimization_fg(features):\n",
    "    best_feature = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for f in features:\n",
    "        bg = BaggingClassifier(mnb, max_samples = bestSampleRatioFG, max_features = f, n_estimators = 200)\n",
    "        bg.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {}\".format(f, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_feature = f\n",
    "    print(\"The best result is {} ratio of sample with accuracy of {}\".format(best_feature, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.6 ratio of sample is 0.4122171945701357\n",
      "Bagging Classifier with 0.7 ratio of sample is 0.41809954751131223\n",
      "Bagging Classifier with 0.8 ratio of sample is 0.4158371040723982\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.4171945701357466\n",
      "Bagging Classifier with 1.0 ratio of sample is 0.4158371040723982\n",
      "The best result is 0.7 ratio of sample with accuracy of 0.41809954751131223\n"
     ]
    }
   ],
   "source": [
    "samples = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "bagging_max_sample_optimization_fg(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best feature ratio so far is 0.7\n",
    "bestFeatureRatioFG = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_estimator_optimization(estimators):\n",
    "    best_estim = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for e in estimators:\n",
    "        bg = BaggingClassifier(mnb, max_samples = bestSampleRatioFG, max_features = bestFeatureRatioFG, n_estimators = e)\n",
    "        bg.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {}\".format(e, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_estim = e\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_estim, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 400 ratio of sample is 0.4149321266968326\n",
      "Bagging Classifier with 600 ratio of sample is 0.4230769230769231\n",
      "Bagging Classifier with 800 ratio of sample is 0.41990950226244345\n",
      "Bagging Classifier with 1000 ratio of sample is 0.42081447963800905\n",
      "The best result is 600 estimators with accuracy of 0.4230769230769231\n"
     ]
    }
   ],
   "source": [
    "estimators = [400, 600, 800, 1000]\n",
    "bagging_max_estimator_optimization(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================AdaBoost Classifier=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 0.9)\n",
    "X_final = cvecGV.transform(X_test_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_max_estimator_optimization_fg(estimators):\n",
    "    best_estim = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for e in estimators:\n",
    "        adb = AdaBoostClassifier(mnb, n_estimators = e, learning_rate = 0.5)\n",
    "        adb.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = adb.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Boosting Classifier with {} ratio of sample is {}\".format(e, result))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_estim = e\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_estim, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 400 ratio of sample is 0.3719457013574661\n",
      "Bagging Classifier with 600 ratio of sample is 0.38280542986425337\n"
     ]
    }
   ],
   "source": [
    "estimators = [400, 600, 800, 1000]\n",
    "boosting_max_estimator_optimization_fg(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=MultinomialNB(alpha=0.5, class_prior=None,\n",
       "                                                fit_prior=True),\n",
       "                   learning_rate=1, n_estimators=400, random_state=None)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adb = AdaBoostClassifier(mnb, n_estimators = 400, learning_rate = 1)\n",
    "adb.fit(X_cvec, y_train_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39728506787330314"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = adb.predict(X_final)\n",
    "accuracy_score(ypred, y_test_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
