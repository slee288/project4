{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import re\n",
    "\n",
    "import string as str\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data with below components removed or replaced\n",
    "url = re.compile(r\"(?:(http[s]?://\\S+)|((//)?(\\w+\\.)?\\w+\\.\\w+/\\S+))\")\n",
    "user_mention = re.compile(r\"(?:(?<!\\w)@\\w+\\b)\")\n",
    "number = re.compile(r\"(?:\\b\\d+\\b)\")\n",
    "repeated_char = '([a-zA-Z])\\\\1+'\n",
    "length_repeated_char = '\\\\1\\\\1'\n",
    "\n",
    "def clean(raw):\n",
    "  #convert HTML encoding to text\n",
    "  new_row = BeautifulSoup(raw, 'html.parser').get_text()\n",
    "  \n",
    "  #Change all text to lower case\n",
    "  new_row = new_row.lower()\n",
    "  \n",
    "  #Replaces any url with class URL\n",
    "  new_row = re.sub(url, '', new_row)\n",
    "  \n",
    "  #replace any @username with class USERNAME\n",
    "  new_row = re.sub(user_mention, '', new_row)\n",
    "  \n",
    "  #Strips repeated chars\n",
    "  new_row = re.sub(repeated_char, length_repeated_char, new_row)\n",
    "  \n",
    "  #Replaces #hashtag with hashtag\n",
    "  new_row = re.sub(r'#(\\S+)', r' \\1 ', new_row)\n",
    "  \n",
    "  #Remove numbers\n",
    "  new_row = re.sub(number, '', new_row)\n",
    "  \n",
    "  #decode text with 'utf-8-sig'\n",
    "  try:\n",
    "    temp_row = new_row.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")     \n",
    "  except:\n",
    "    temp_row = new_row\n",
    "  \n",
    "  #Removes emojis\n",
    "#   new_row = handle_emojis(temp_row);\n",
    "  \n",
    "  return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>1</td>\n",
       "      <td>226166</td>\n",
       "      <td>0.69444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>226300</td>\n",
       "      <td>0.83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>1</td>\n",
       "      <td>225801</td>\n",
       "      <td>0.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>1</td>\n",
       "      <td>14646</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>1</td>\n",
       "      <td>14644</td>\n",
       "      <td>0.72222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8539</td>\n",
       "      <td>8539</td>\n",
       "      <td>11851</td>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>1</td>\n",
       "      <td>222071</td>\n",
       "      <td>0.11111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8540</td>\n",
       "      <td>8540</td>\n",
       "      <td>11852</td>\n",
       "      <td>No surprises .</td>\n",
       "      <td>1</td>\n",
       "      <td>225165</td>\n",
       "      <td>0.22222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8541</td>\n",
       "      <td>8541</td>\n",
       "      <td>11853</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>1</td>\n",
       "      <td>226985</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8542</td>\n",
       "      <td>8542</td>\n",
       "      <td>11854</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>223632</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8543</td>\n",
       "      <td>8543</td>\n",
       "      <td>11855</td>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>1</td>\n",
       "      <td>224044</td>\n",
       "      <td>0.34722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  sentence_index  \\\n",
       "0              0               1   \n",
       "1              1               2   \n",
       "2              2              61   \n",
       "3              3              62   \n",
       "4              4              63   \n",
       "...          ...             ...   \n",
       "8539        8539           11851   \n",
       "8540        8540           11852   \n",
       "8541        8541           11853   \n",
       "8542        8542           11854   \n",
       "8543        8543           11855   \n",
       "\n",
       "                                               sentence  splitset_label  \\\n",
       "0     The Rock is destined to be the 21st Century 's...               1   \n",
       "1     The gorgeously elaborate continuation of `` Th...               1   \n",
       "2     Singer\\/composer Bryan Adams contributes a sle...               1   \n",
       "3     You 'd think by now America would have had eno...               1   \n",
       "4                  Yet the act is still charming here .               1   \n",
       "...                                                 ...             ...   \n",
       "8539                                    A real snooze .               1   \n",
       "8540                                     No surprises .               1   \n",
       "8541  We 've seen the hippie-turned-yuppie plot befo...               1   \n",
       "8542  Her fans walked out muttering words like `` ho...               1   \n",
       "8543                                In this case zero .               1   \n",
       "\n",
       "          id   values  \n",
       "0     226166  0.69444  \n",
       "1     226300  0.83333  \n",
       "2     225801  0.62500  \n",
       "3      14646  0.50000  \n",
       "4      14644  0.72222  \n",
       "...      ...      ...  \n",
       "8539  222071  0.11111  \n",
       "8540  225165  0.22222  \n",
       "8541  226985  0.75000  \n",
       "8542  223632  0.13889  \n",
       "8543  224044  0.34722  \n",
       "\n",
       "[8544 rows x 6 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw = pd.read_csv(\"SSTB_train.csv\")\n",
    "df_train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>2</td>\n",
       "      <td>13995</td>\n",
       "      <td>0.513890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>2</td>\n",
       "      <td>14123</td>\n",
       "      <td>0.736110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>13999</td>\n",
       "      <td>0.861110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>14498</td>\n",
       "      <td>0.597220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>2</td>\n",
       "      <td>14351</td>\n",
       "      <td>0.833330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2205</td>\n",
       "      <td>2205</td>\n",
       "      <td>11621</td>\n",
       "      <td>An imaginative comedy\\/thriller .</td>\n",
       "      <td>2</td>\n",
       "      <td>13851</td>\n",
       "      <td>0.777780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2206</td>\n",
       "      <td>2206</td>\n",
       "      <td>11623</td>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "      <td>2</td>\n",
       "      <td>18182</td>\n",
       "      <td>0.916670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2207</td>\n",
       "      <td>2207</td>\n",
       "      <td>11626</td>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "      <td>2</td>\n",
       "      <td>23211</td>\n",
       "      <td>0.888890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2208</td>\n",
       "      <td>2208</td>\n",
       "      <td>11628</td>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "      <td>2</td>\n",
       "      <td>26177</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2209</td>\n",
       "      <td>2209</td>\n",
       "      <td>11798</td>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "      <td>2</td>\n",
       "      <td>141831</td>\n",
       "      <td>0.069444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2210 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  sentence_index  \\\n",
       "0              0               3   \n",
       "1              1               4   \n",
       "2              2               5   \n",
       "3              3               6   \n",
       "4              4               7   \n",
       "...          ...             ...   \n",
       "2205        2205           11621   \n",
       "2206        2206           11623   \n",
       "2207        2207           11626   \n",
       "2208        2208           11628   \n",
       "2209        2209           11798   \n",
       "\n",
       "                                               sentence  splitset_label  \\\n",
       "0                        Effective but too-tepid biopic               2   \n",
       "1     If you sometimes like to go to the movies to h...               2   \n",
       "2     Emerges as something rare , an issue movie tha...               2   \n",
       "3     The film provides some great insight into the ...               2   \n",
       "4     Offers that rare combination of entertainment ...               2   \n",
       "...                                                 ...             ...   \n",
       "2205                  An imaginative comedy\\/thriller .               2   \n",
       "2206                      ( A ) rare , beautiful film .               2   \n",
       "2207                 ( An ) hilarious romantic comedy .               2   \n",
       "2208                Never ( sinks ) into exploitation .               2   \n",
       "2209                        ( U ) nrelentingly stupid .               2   \n",
       "\n",
       "          id    values  \n",
       "0      13995  0.513890  \n",
       "1      14123  0.736110  \n",
       "2      13999  0.861110  \n",
       "3      14498  0.597220  \n",
       "4      14351  0.833330  \n",
       "...      ...       ...  \n",
       "2205   13851  0.777780  \n",
       "2206   18182  0.916670  \n",
       "2207   23211  0.888890  \n",
       "2208   26177  0.625000  \n",
       "2209  141831  0.069444  \n",
       "\n",
       "[2210 rows x 6 columns]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_raw = pd.read_csv(\"SSTB_test.csv\")\n",
    "df_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = []\n",
    "clean_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 8544):\n",
    "    clean_train.append(clean(df_train_raw['sentence'][i]))\n",
    "\n",
    "for i in range(0, 2210):\n",
    "    clean_test.append(clean(df_test_raw['sentence'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv file\n",
    "clean_df_train = pd.DataFrame(clean_train, columns=['sentence'])\n",
    "clean_df_test = pd.DataFrame(clean_test, columns = ['sentence'])\n",
    "\n",
    "clean_df_train['values'] = df_train_raw[\"values\"]\n",
    "clean_df_test['values'] = df_test_raw[\"values\"]\n",
    "\n",
    "clean_df_train.to_csv('SSTB_clean_train.csv',encoding='utf-8')\n",
    "clean_df_test.to_csv(\"SSTB_clean_test.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>the rock is destined to be the 21st century 's...</td>\n",
       "      <td>0.69444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the gorgeously elaborate continuation of `` th...</td>\n",
       "      <td>0.83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>singer\\/composer bryan adams contributes a sle...</td>\n",
       "      <td>0.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>you 'd think by now america would have had eno...</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yet the act is still charming here .</td>\n",
       "      <td>0.72222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8539</td>\n",
       "      <td>a real snooze .</td>\n",
       "      <td>0.11111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8540</td>\n",
       "      <td>no surprises .</td>\n",
       "      <td>0.22222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8541</td>\n",
       "      <td>we 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8542</td>\n",
       "      <td>her fans walked out muttering words like `` ho...</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8543</td>\n",
       "      <td>in this case zero .</td>\n",
       "      <td>0.34722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence   values\n",
       "0     the rock is destined to be the 21st century 's...  0.69444\n",
       "1     the gorgeously elaborate continuation of `` th...  0.83333\n",
       "2     singer\\/composer bryan adams contributes a sle...  0.62500\n",
       "3     you 'd think by now america would have had eno...  0.50000\n",
       "4                  yet the act is still charming here .  0.72222\n",
       "...                                                 ...      ...\n",
       "8539                                    a real snooze .  0.11111\n",
       "8540                                     no surprises .  0.22222\n",
       "8541  we 've seen the hippie-turned-yuppie plot befo...  0.75000\n",
       "8542  her fans walked out muttering words like `` ho...  0.13889\n",
       "8543                                in this case zero .  0.34722\n",
       "\n",
       "[8544 rows x 2 columns]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using columns 1,2 because col 0 is the number of the row, could be changed if the row number is removed from the csv file.\n",
    "df_train = pd.read_csv('SSTB_clean_train.csv', usecols = [1,2], encoding='latin-1')\n",
    "df_test = pd.read_csv(\"SSTB_clean_test.csv\", usecols = [1,2], encoding = \"latin-1\")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting the y values to positive or negative values\n",
    "\n",
    "def convert_to_pos_neg(y):\n",
    "    for i in range(len(y)):\n",
    "        if y[i] >= 3:\n",
    "            y[i] = 1\n",
    "        elif y[i] <= 1:\n",
    "            y[i] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_fine_grain(y):\n",
    "    for i in range(len(y)):\n",
    "        if y[i] >= 0 and y[i] <= 0.2:\n",
    "            y[i] = 0\n",
    "        elif y[i] > 0.2 and y[i] < 0.4:\n",
    "            y[i] = 1\n",
    "        elif y[i] > 0.4 and y[i] < 0.6:\n",
    "            y[i] = 2\n",
    "        elif y[i] > 0.6 and y[i] < 0.8:\n",
    "            y[i] = 3\n",
    "        elif y[i] > 0.8 and y[i] <= 1.0:\n",
    "            y[i] = 4\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset labels into fine grain classification\n",
    "df_train[\"values\"] = convert_to_fine_grain(df_train[\"values\"])\n",
    "df_test[\"values\"] = convert_to_fine_grain(df_test[\"values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into positive / negative binary classification\n",
    "# In order to not to alter the original dataset, we make a copy of each\n",
    "df_train_copy = df_train.copy()\n",
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>the rock is destined to be the 21st century 's...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the gorgeously elaborate continuation of `` th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>singer\\/composer bryan adams contributes a sle...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>you 'd think by now america would have had eno...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yet the act is still charming here .</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8539</td>\n",
       "      <td>a real snooze .</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8540</td>\n",
       "      <td>no surprises .</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8541</td>\n",
       "      <td>we 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8542</td>\n",
       "      <td>her fans walked out muttering words like `` ho...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8543</td>\n",
       "      <td>in this case zero .</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  values\n",
       "0     the rock is destined to be the 21st century 's...     1.0\n",
       "1     the gorgeously elaborate continuation of `` th...     1.0\n",
       "2     singer\\/composer bryan adams contributes a sle...     1.0\n",
       "3     you 'd think by now america would have had eno...     2.0\n",
       "4                  yet the act is still charming here .     1.0\n",
       "...                                                 ...     ...\n",
       "8539                                    a real snooze .     0.0\n",
       "8540                                     no surprises .     0.0\n",
       "8541  we 've seen the hippie-turned-yuppie plot befo...     1.0\n",
       "8542  her fans walked out muttering words like `` ho...     0.0\n",
       "8543                                in this case zero .     0.0\n",
       "\n",
       "[8544 rows x 2 columns]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Firstly, convert the target values into positive / negative, except for neutral ones\n",
    "# which will be removed after\n",
    "df_train_copy[\"values\"] = convert_to_pos_neg(df_train_copy[\"values\"])\n",
    "df_test_copy[\"values\"] = convert_to_pos_neg(df_test_copy[\"values\"])\n",
    "df_train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>offers that rare combination of entertainment ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>perhaps no picture ever made has more literall...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>steers turns in a snappy screenplay that curls...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2205</td>\n",
       "      <td>an imaginative comedy\\/thriller .</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2206</td>\n",
       "      <td>( a ) rare , beautiful film .</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2207</td>\n",
       "      <td>( an ) hilarious romantic comedy .</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2208</td>\n",
       "      <td>never ( sinks ) into exploitation .</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2209</td>\n",
       "      <td>( u ) nrelentingly stupid .</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  values\n",
       "1     if you sometimes like to go to the movies to h...     1.0\n",
       "2     emerges as something rare , an issue movie tha...     1.0\n",
       "4     offers that rare combination of entertainment ...     1.0\n",
       "5     perhaps no picture ever made has more literall...     1.0\n",
       "6     steers turns in a snappy screenplay that curls...     1.0\n",
       "...                                                 ...     ...\n",
       "2205                  an imaginative comedy\\/thriller .     1.0\n",
       "2206                      ( a ) rare , beautiful film .     1.0\n",
       "2207                 ( an ) hilarious romantic comedy .     1.0\n",
       "2208                never ( sinks ) into exploitation .     1.0\n",
       "2209                        ( u ) nrelentingly stupid .     0.0\n",
       "\n",
       "[1821 rows x 2 columns]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the neutral values for positive / negative classification\n",
    "df_train_copy.drop(df_train_copy[df_train_copy[\"values\"] == 2].index, inplace=True)\n",
    "df_test_copy.drop(df_test_copy[df_test_copy[\"values\"] == 2].index, inplace=True)\n",
    "df_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_posneg = df_train_copy[\"sentence\"]\n",
    "y_train_posneg = df_train_copy[\"values\"]\n",
    "\n",
    "X_test_posneg = df_test_copy[\"sentence\"]\n",
    "y_test_posneg = df_test_copy[\"values\"]\n",
    "\n",
    "X_train_finegrain = df_train[\"sentence\"]\n",
    "y_train_finegrain = df_train[\"values\"]\n",
    "\n",
    "X_test_finegrain = df_test[\"sentence\"]\n",
    "y_test_finegrain = df_test[\"values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_posneg_cvec = cvec.fit_transform(X_train_posneg, y_train_posneg)\n",
    "X_train_posneg_tf = tfidf.fit_transform(X_train_posneg, y_train_posneg)\n",
    "\n",
    "X_test_posneg_cvec = cvec.transform(X_test_posneg)\n",
    "X_test_posneg_tf = tfidf.transform(X_test_posneg)\n",
    "\n",
    "X_train_finegrain_cvec = cvec.fit_transform(X_train_finegrain, y_train_finegrain)\n",
    "X_train_finegrain_tf = tfidf.fit_transform(X_train_finegrain, y_train_finegrain)\n",
    "\n",
    "X_test_finegrain_cvec = cvec.transform(X_test_finegrain)\n",
    "X_test_finegrain_tf = tfidf.transform(X_test_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation with countvectorizer:  0.7929190751445085\n",
      "Cross validation with TfidfVectorizer:  0.7880057803468208\n",
      "Time Execution: 0.08872743800020544\n"
     ]
    }
   ],
   "source": [
    "#=====================Generic 5-fold Cross Validation Score On Classifying Positive/Negative==================\n",
    "# This cross validation will only operate on the training set\n",
    "# Start the Timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "results_cv = model_selection.cross_val_score(mnb, X_train_posneg_cvec, y_train_posneg, cv = 5)\n",
    "print(\"Cross validation with countvectorizer: \", results_cv.mean())\n",
    "\n",
    "results_tf = model_selection.cross_val_score(mnb, X_train_posneg_tf, y_train_posneg, cv = 5)\n",
    "print(\"Cross validation with TfidfVectorizer: \", results_tf.mean())\n",
    "# mnb.fit(X_train_tf, y_train)\n",
    "# y_pred = mnb.predict(X_test_tf)\n",
    "\n",
    "# print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "# Timer stops\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time Execution: {}\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation with countvectorizer:  0.3957121777381964\n",
      "Cross validation with TfidfVectorizer:  0.3957132081460396\n",
      "Time Execution: 0.14975032799702603\n"
     ]
    }
   ],
   "source": [
    "#=====================Generic 5-fold Cross Validation Score on Classifying Fine Grain==================\n",
    "# This cross validation will only operate on the training set\n",
    "# Start the Timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "results_cv = model_selection.cross_val_score(mnb, X_train_finegrain_cvec, y_train_finegrain, cv = 5)\n",
    "print(\"Cross validation with countvectorizer: \", results_cv.mean())\n",
    "\n",
    "results_tf = model_selection.cross_val_score(mnb, X_train_finegrain_tf, y_train_finegrain, cv = 5)\n",
    "print(\"Cross validation with TfidfVectorizer: \", results_tf.mean())\n",
    "# mnb.fit(X_train_tf, y_train)\n",
    "# y_pred = mnb.predict(X_test_tf)\n",
    "\n",
    "# print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "# Timer stops\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time Execution: {}\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Positive/Negative Classification========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================Finding Optimal N_gram============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the accuracy of a given classifier and vectorizer\n",
    "def nfeature_accuracy_checker(X_train, y_train, X_test, y_test, vectorizer=None, n_features=None, stop_words=None, ngram_range=(1, 1), classifier=None):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        t0 = time()\n",
    "        sentiment_fit = checker_pipeline.fit(X_train, y_train)\n",
    "        y_pred = sentiment_fit.predict(X_test)\n",
    "        train_test_time = time() - t0\n",
    "        accuracy = accuracy_score(y_pred, y_test)\n",
    "        print(\"accuracy with\", n, \"features:\", accuracy, \"time:\", train_test_time)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_extractor = [None, \"english\"]\n",
    "vectorizer = [cvec, tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 10000 features: 0.814936847885777 time: 0.24288511276245117\n",
      "accuracy with 20000 features: 0.8160351455244371 time: 0.2191309928894043\n",
      "accuracy with 30000 features: 0.8160351455244371 time: 0.2085590362548828\n",
      "accuracy with 40000 features: 0.8160351455244371 time: 0.2638392448425293\n",
      "accuracy with 50000 features: 0.8160351455244371 time: 0.2991969585418701\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 10000 features: 0.8160351455244371 time: 0.2213451862335205\n",
      "accuracy with 20000 features: 0.8088962108731467 time: 0.23039674758911133\n",
      "accuracy with 30000 features: 0.8088962108731467 time: 0.22635698318481445\n",
      "accuracy with 40000 features: 0.8088962108731467 time: 0.22919416427612305\n",
      "accuracy with 50000 features: 0.8088962108731467 time: 0.25876474380493164\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 10000 features: 0.8034047226798462 time: 0.21070384979248047\n",
      "accuracy with 20000 features: 0.8099945085118067 time: 0.2010197639465332\n",
      "accuracy with 30000 features: 0.8099945085118067 time: 0.20353102684020996\n",
      "accuracy with 40000 features: 0.8099945085118067 time: 0.21084117889404297\n",
      "accuracy with 50000 features: 0.8099945085118067 time: 0.3072359561920166\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 10000 features: 0.8023064250411862 time: 0.20152807235717773\n",
      "accuracy with 20000 features: 0.800109829763866 time: 0.21880030632019043\n",
      "accuracy with 30000 features: 0.800109829763866 time: 0.31673717498779297\n",
      "accuracy with 40000 features: 0.800109829763866 time: 0.25997185707092285\n",
      "accuracy with 50000 features: 0.800109829763866 time: 0.23598599433898926\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_features = np.arange(10000,50001,10000)\n",
    "\n",
    "# unigram case \n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_posneg, y_train_posneg, X_test_posneg, y_test_posneg,\n",
    "                                                                                   vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 30000 features: 0.8165842943437671 time: 0.7560849189758301\n",
      "accuracy with 40000 features: 0.8193300384404174 time: 0.7224512100219727\n",
      "accuracy with 50000 features: 0.8198791872597474 time: 0.6592221260070801\n",
      "accuracy with 60000 features: 0.8182317408017573 time: 0.6419909000396729\n",
      "accuracy with 70000 features: 0.8182317408017573 time: 0.6413350105285645\n",
      "accuracy with 80000 features: 0.8226249313563976 time: 0.640186071395874\n",
      "accuracy with 90000 features: 0.8220757825370676 time: 0.638437032699585\n",
      "accuracy with 100000 features: 0.8220757825370676 time: 0.6400349140167236\n",
      "accuracy with 110000 features: 0.8220757825370676 time: 0.6420121192932129\n",
      "accuracy with 120000 features: 0.8220757825370676 time: 0.6386301517486572\n",
      "accuracy with 130000 features: 0.8220757825370676 time: 0.7007782459259033\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 30000 features: 0.8160351455244371 time: 0.7270867824554443\n",
      "accuracy with 40000 features: 0.8165842943437671 time: 0.7299280166625977\n",
      "accuracy with 50000 features: 0.8138385502471169 time: 0.72757887840271\n",
      "accuracy with 60000 features: 0.8105436573311368 time: 0.6754491329193115\n",
      "accuracy with 70000 features: 0.8094453596924767 time: 0.7835729122161865\n",
      "accuracy with 80000 features: 0.8083470620538166 time: 0.7574818134307861\n",
      "accuracy with 90000 features: 0.8077979132344866 time: 0.7263619899749756\n",
      "accuracy with 100000 features: 0.8077979132344866 time: 0.6828711032867432\n",
      "accuracy with 110000 features: 0.8077979132344866 time: 0.6834230422973633\n",
      "accuracy with 120000 features: 0.8077979132344866 time: 0.6803090572357178\n",
      "accuracy with 130000 features: 0.8077979132344866 time: 0.7384529113769531\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 30000 features: 0.7984623833058759 time: 0.5185258388519287\n",
      "accuracy with 40000 features: 0.7995606809445359 time: 0.5230872631072998\n",
      "accuracy with 50000 features: 0.7995606809445359 time: 0.49296998977661133\n",
      "accuracy with 60000 features: 0.7979132344865458 time: 0.49327516555786133\n",
      "accuracy with 70000 features: 0.800109829763866 time: 0.49747800827026367\n",
      "accuracy with 80000 features: 0.800109829763866 time: 0.5509381294250488\n",
      "accuracy with 90000 features: 0.800109829763866 time: 0.4886760711669922\n",
      "accuracy with 100000 features: 0.800109829763866 time: 0.4777250289916992\n",
      "accuracy with 110000 features: 0.800109829763866 time: 0.49446916580200195\n",
      "accuracy with 120000 features: 0.800109829763866 time: 0.5263817310333252\n",
      "accuracy with 130000 features: 0.800109829763866 time: 0.48578619956970215\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 30000 features: 0.7968149368478857 time: 0.5144038200378418\n",
      "accuracy with 40000 features: 0.7957166392092258 time: 0.7178080081939697\n",
      "accuracy with 50000 features: 0.800658978583196 time: 0.5729339122772217\n",
      "accuracy with 60000 features: 0.7946183415705657 time: 0.616689920425415\n",
      "accuracy with 70000 features: 0.7929708951125755 time: 0.5033910274505615\n",
      "accuracy with 80000 features: 0.7929708951125755 time: 0.5676472187042236\n",
      "accuracy with 90000 features: 0.7929708951125755 time: 0.5515880584716797\n",
      "accuracy with 100000 features: 0.7929708951125755 time: 0.5699129104614258\n",
      "accuracy with 110000 features: 0.7929708951125755 time: 0.49767398834228516\n",
      "accuracy with 120000 features: 0.7929708951125755 time: 0.5663559436798096\n",
      "accuracy with 130000 features: 0.7929708951125755 time: 0.49604225158691406\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram case \n",
    "# in here, we modify the number of features from the range 30000 to 130000, since \n",
    "# much more features will be created\n",
    "\n",
    "n_features = np.arange(30000,130001,10000)\n",
    "\n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_posneg, y_train_posneg, X_test_posneg, y_test_posneg,\n",
    "                                                                vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], ngram_range = (1, 2), classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 60000 features: 0.814387699066447 time: 1.2241559028625488\n",
      "accuracy with 70000 features: 0.8121911037891268 time: 1.385418176651001\n",
      "accuracy with 80000 features: 0.814387699066447 time: 1.1598639488220215\n",
      "accuracy with 90000 features: 0.8154859967051071 time: 1.284280776977539\n",
      "accuracy with 100000 features: 0.814936847885777 time: 1.4914159774780273\n",
      "accuracy with 110000 features: 0.8171334431630972 time: 1.2772462368011475\n",
      "accuracy with 120000 features: 0.8165842943437671 time: 1.210932970046997\n",
      "accuracy with 130000 features: 0.8176825919824272 time: 1.3571701049804688\n",
      "accuracy with 140000 features: 0.8154859967051071 time: 1.2905611991882324\n",
      "accuracy with 150000 features: 0.8116419549697969 time: 1.1901488304138184\n",
      "accuracy with 160000 features: 0.8132894014277869 time: 1.2022037506103516\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 60000 features: 0.8066996155958265 time: 1.1755242347717285\n",
      "accuracy with 70000 features: 0.8050521691378364 time: 1.1875560283660889\n",
      "accuracy with 80000 features: 0.8066996155958265 time: 1.3811159133911133\n",
      "accuracy with 90000 features: 0.8050521691378364 time: 1.3801448345184326\n",
      "accuracy with 100000 features: 0.8061504667764964 time: 1.5991742610931396\n",
      "accuracy with 110000 features: 0.8077979132344866 time: 1.2721219062805176\n",
      "accuracy with 120000 features: 0.8056013179571664 time: 1.2634427547454834\n",
      "accuracy with 130000 features: 0.8045030203185063 time: 1.1967811584472656\n",
      "accuracy with 140000 features: 0.8012081274025261 time: 1.2342007160186768\n",
      "accuracy with 150000 features: 0.7984623833058759 time: 1.2535679340362549\n",
      "accuracy with 160000 features: 0.7973640856672158 time: 1.4615778923034668\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 60000 features: 0.7940691927512356 time: 0.8590719699859619\n",
      "accuracy with 70000 features: 0.7984623833058759 time: 0.8348066806793213\n",
      "accuracy with 80000 features: 0.8017572762218561 time: 1.044645071029663\n",
      "accuracy with 90000 features: 0.800109829763866 time: 0.9345729351043701\n",
      "accuracy with 100000 features: 0.800658978583196 time: 0.9446420669555664\n",
      "accuracy with 110000 features: 0.7968149368478857 time: 0.8217451572418213\n",
      "accuracy with 120000 features: 0.7968149368478857 time: 0.7656269073486328\n",
      "accuracy with 130000 features: 0.7968149368478857 time: 0.7612128257751465\n",
      "accuracy with 140000 features: 0.7968149368478857 time: 0.7764668464660645\n",
      "accuracy with 150000 features: 0.7968149368478857 time: 0.8346009254455566\n",
      "accuracy with 160000 features: 0.7968149368478857 time: 0.9237709045410156\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 60000 features: 0.7907742998352554 time: 0.8104557991027832\n",
      "accuracy with 70000 features: 0.7929708951125755 time: 0.8245806694030762\n",
      "accuracy with 80000 features: 0.7946183415705657 time: 0.9968240261077881\n",
      "accuracy with 90000 features: 0.7940691927512356 time: 0.8460650444030762\n",
      "accuracy with 100000 features: 0.7907742998352554 time: 1.008242130279541\n",
      "accuracy with 110000 features: 0.7918725974739155 time: 0.858231782913208\n",
      "accuracy with 120000 features: 0.7918725974739155 time: 0.9211947917938232\n",
      "accuracy with 130000 features: 0.7918725974739155 time: 0.9781539440155029\n",
      "accuracy with 140000 features: 0.7918725974739155 time: 0.8813309669494629\n",
      "accuracy with 150000 features: 0.7918725974739155 time: 0.9137899875640869\n",
      "accuracy with 160000 features: 0.7918725974739155 time: 0.9057388305664062\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram + trigram case\n",
    "# We furthur extent the number of features to the range of 120000\n",
    "\n",
    "n_features = np.arange(60000,160001,10000)\n",
    "\n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_posneg, y_train_posneg, X_test_posneg, y_test_posneg,\n",
    "                                                                vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], ngram_range = (1, 3), classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, the bigram case with CountVectorizer 80000 features has the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnbGV = MultinomialNB()\n",
    "\n",
    "\n",
    "cvecGV = CountVectorizer(ngram_range = (1, 2), max_features = 80000)\n",
    "X_cvec = cvecGV.fit_transform(X_train_posneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning(alphas):\n",
    "    best_alpha = None\n",
    "    best_result = 0\n",
    "    \n",
    "\n",
    "    for a in alphas:\n",
    "        mnb_model = MultinomialNB(alpha = a)\n",
    "        t0 = time()\n",
    "        X_final = cvecGV.transform(X_test_posneg)\n",
    "        mnb_model.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = mnb_model.predict(X_final)\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        train_test_time = time() - t0\n",
    "        print(\"accuracy for alpha =\", a, \":\", result, \"time:\", train_test_time)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_alpha = a\n",
    "        \n",
    "    print(\"The best result is\",best_result,\"with alpha of\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for alpha = 0.7 : 0.8209774848984075 time: 0.11713171005249023\n",
      "accuracy for alpha = 0.8 : 0.8193300384404174 time: 0.08710002899169922\n",
      "accuracy for alpha = 0.9 : 0.8215266337177375 time: 0.08791804313659668\n",
      "accuracy for alpha = 1.0 : 0.8226249313563976 time: 0.09646797180175781\n",
      "accuracy for alpha = 1.1 : 0.8198791872597474 time: 0.08778977394104004\n",
      "accuracy for alpha = 1.2 : 0.8193300384404174 time: 0.08960604667663574\n",
      "The best result is 0.8226249313563976 with alpha of 1.0\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.7, 0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "hyper_parameter_tuning(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparameter value so far is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================Bagging Classifier with NB Model================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Timer begins\n",
    "# start = timeit.default_timer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 1.0)\n",
    "X_final = cvecGV.transform(X_test_posneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_sample_optimization(samples):\n",
    "    best_sample = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for s in samples:\n",
    "        bg = BaggingClassifier(mnb, max_samples = s, max_features = 0.5, n_estimators = 200)\n",
    "        t0 = time()\n",
    "        bg.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {} with the time of {}\".format(s, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_sample = s\n",
    "    print(\"The best result is {} ratio of sample with accuracy of {}\".format(best_sample, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.5 ratio of sample is 0.8127402526084568 with the time of 4.0490710735321045\n",
      "Bagging Classifier with 0.6 ratio of sample is 0.8099945085118067 with the time of 4.096158027648926\n",
      "Bagging Classifier with 0.7 ratio of sample is 0.8176825919824272 with the time of 3.7774651050567627\n",
      "Bagging Classifier with 0.8 ratio of sample is 0.8165842943437671 with the time of 4.001101970672607\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.8154859967051071 with the time of 3.8665168285369873\n",
      "Bagging Classifier with 1.0 ratio of sample is 0.8187808896210873 with the time of 4.048224210739136\n",
      "The best result is 1.0 ratio of sample with accuracy of 0.8187808896210873\n"
     ]
    }
   ],
   "source": [
    "samples = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "bagging_max_sample_optimization(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best ratio is so far is 0.9, so we set the best sample ratio as a variable\n",
    "bestSampleRate = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_feature_optimization(features):\n",
    "    best_feature = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for f in features:\n",
    "        bg = BaggingClassifier(mnb, max_samples = bestSampleRate, max_features = f, n_estimators = 200)\n",
    "        t0 = time()\n",
    "        bg.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {} with the time of {}\".format(f, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_feature = f\n",
    "    print(\"The best result is {} ratio of sample with accuracy of {}\".format(best_feature, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.4 ratio of sample is 0.8160351455244371 with the time of 3.7128469944000244\n",
      "Bagging Classifier with 0.5 ratio of sample is 0.8209774848984075 with the time of 3.822340726852417\n",
      "Bagging Classifier with 0.6 ratio of sample is 0.8187808896210873 with the time of 4.365931987762451\n",
      "Bagging Classifier with 0.7 ratio of sample is 0.8187808896210873 with the time of 5.061239004135132\n",
      "Bagging Classifier with 0.8 ratio of sample is 0.8215266337177375 with the time of 5.336562156677246\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.8193300384404174 with the time of 5.809247970581055\n",
      "The best result is 0.8 ratio of sample with accuracy of 0.8215266337177375\n"
     ]
    }
   ],
   "source": [
    "features = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "bagging_max_feature_optimization(features)\n",
    "\n",
    "# # Timer stops\n",
    "# stop = timeit.default_timer()\n",
    "# print(\"Time Execution bagging classifier: {}\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatureRatio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_estimator_optimization(estimators):\n",
    "    best_estim = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for e in estimators:\n",
    "        bg = BaggingClassifier(mnb, max_samples = bestSampleRate, max_features = bestFeatureRatio, n_estimators = e)\n",
    "        t0 = time()\n",
    "        bg.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {} with the time of {}\".format(e, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_estim = e\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_estim, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 200 ratio of sample is 0.8171334431630972 with the time of 5.558647155761719\n",
      "Bagging Classifier with 400 ratio of sample is 0.8154859967051071 with the time of 11.622493982315063\n",
      "Bagging Classifier with 600 ratio of sample is 0.8171334431630972 with the time of 17.5408718585968\n",
      "Bagging Classifier with 800 ratio of sample is 0.8171334431630972 with the time of 22.187037229537964\n",
      "Bagging Classifier with 1000 ratio of sample is 0.8193300384404174 with the time of 37.463013887405396\n",
      "The best result is 1000 estimators with accuracy of 0.8193300384404174\n"
     ]
    }
   ],
   "source": [
    "estimators = [200, 400, 600, 800, 1000]\n",
    "bagging_max_estimator_optimization(estimators)\n",
    "\n",
    "# This is the final result of mnb with all the techniques above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== AdaBoost Optimization===================\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 0.9)\n",
    "X_final = cvecGV.transform(X_test_posneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_max_estimator_optimization(estimators):\n",
    "    best_estim = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for e in estimators:\n",
    "        adb = AdaBoostClassifier(mnb, n_estimators = e, learning_rate = 1.2)\n",
    "        t0 = time()\n",
    "        adb.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = adb.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Boosting Classifier with {} ratio of sample is {} with the time of {}\".format(e, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_estim = e\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_estim, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Classifier with 200 ratio of sample is 0.6996155958264689 with the time of 3.0156829357147217\n",
      "Boosting Classifier with 400 ratio of sample is 0.7841845140032949 with the time of 5.851296901702881\n",
      "Boosting Classifier with 600 ratio of sample is 0.800658978583196 with the time of 9.773756265640259\n",
      "Boosting Classifier with 800 ratio of sample is 0.8017572762218561 with the time of 12.311228275299072\n",
      "Boosting Classifier with 1000 ratio of sample is 0.8017572762218561 with the time of 11.153487920761108\n",
      "The best result is 800 estimators with accuracy of 0.8017572762218561\n"
     ]
    }
   ],
   "source": [
    "estimators = [200, 400, 600, 800, 1000]\n",
    "boosting_max_estimator_optimization(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestEstimators = 600\n",
    "\n",
    "def boosting_best_lr_optimization(learningrates):\n",
    "    best_lr = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for lr in learningrates:\n",
    "        adb = AdaBoostClassifier(mnb, n_estimators = bestEstimators, learning_rate = lr)\n",
    "        t0 = time()\n",
    "        adb.fit(X_cvec, y_train_posneg)\n",
    "        y_pred = adb.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_posneg)\n",
    "        print(\"Boosting Classifier with {} ratio of sample is {} with the time of {}\".format(lr, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_lr= lr\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_lr, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Classifier with 0.8 ratio of sample is 0.7869302580999451 with the time of 6.500154733657837\n",
      "Boosting Classifier with 1 ratio of sample is 0.786381109280615 with the time of 5.996299982070923\n",
      "Boosting Classifier with 1.1 ratio of sample is 0.7913234486545854 with the time of 6.469648122787476\n",
      "Boosting Classifier with 1.2 ratio of sample is 0.800658978583196 with the time of 9.436261892318726\n",
      "Boosting Classifier with 1.3 ratio of sample is 0.7896760021965953 with the time of 9.451599836349487\n",
      "The best result is 1.2 estimators with accuracy of 0.800658978583196\n"
     ]
    }
   ],
   "source": [
    "learningrates = [0.8, 1, 1.1, 1.2, 1.3]\n",
    "boosting_best_lr_optimization(learningrates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================End of Positive / Negative Section=================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================Fine Grain Section==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_extractor = [None, \"english\"]\n",
    "vectorizer = [cvec, tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.41357466063348414 time: 0.31301403045654297\n",
      "accuracy with 5000 features: 0.4176470588235294 time: 0.27707886695861816\n",
      "accuracy with 7500 features: 0.4149321266968326 time: 0.2858729362487793\n",
      "accuracy with 10000 features: 0.4085972850678733 time: 0.28822875022888184\n",
      "accuracy with 12500 features: 0.4108597285067873 time: 0.3307499885559082\n",
      "accuracy with 15000 features: 0.4117647058823529 time: 0.31243109703063965\n",
      "accuracy with 17500 features: 0.4113122171945701 time: 0.27562594413757324\n",
      "accuracy with 20000 features: 0.4113122171945701 time: 0.27642393112182617\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.40090497737556563 time: 0.28070712089538574\n",
      "accuracy with 5000 features: 0.40316742081447965 time: 0.27507996559143066\n",
      "accuracy with 7500 features: 0.4 time: 0.2819690704345703\n",
      "accuracy with 10000 features: 0.40316742081447965 time: 0.28792476654052734\n",
      "accuracy with 12500 features: 0.4 time: 0.3312828540802002\n",
      "accuracy with 15000 features: 0.39683257918552034 time: 0.31558990478515625\n",
      "accuracy with 17500 features: 0.39683257918552034 time: 0.321688175201416\n",
      "accuracy with 20000 features: 0.39683257918552034 time: 0.3276233673095703\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.38506787330316744 time: 0.26012206077575684\n",
      "accuracy with 5000 features: 0.39547511312217193 time: 0.3189990520477295\n",
      "accuracy with 7500 features: 0.39638009049773754 time: 0.30126309394836426\n",
      "accuracy with 10000 features: 0.39683257918552034 time: 0.26534223556518555\n",
      "accuracy with 12500 features: 0.39592760180995473 time: 0.261336088180542\n",
      "accuracy with 15000 features: 0.3932126696832579 time: 0.30584120750427246\n",
      "accuracy with 17500 features: 0.3932126696832579 time: 0.2732579708099365\n",
      "accuracy with 20000 features: 0.3932126696832579 time: 0.24918007850646973\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.38280542986425337 time: 0.3164710998535156\n",
      "accuracy with 5000 features: 0.39728506787330314 time: 0.2756969928741455\n",
      "accuracy with 7500 features: 0.39638009049773754 time: 0.2508819103240967\n",
      "accuracy with 10000 features: 0.3918552036199095 time: 0.26920032501220703\n",
      "accuracy with 12500 features: 0.3945701357466063 time: 0.2531130313873291\n",
      "accuracy with 15000 features: 0.3936651583710407 time: 0.290355920791626\n",
      "accuracy with 17500 features: 0.3936651583710407 time: 0.3426036834716797\n",
      "accuracy with 20000 features: 0.3936651583710407 time: 0.26824402809143066\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_features = np.arange(2500,20001,2500)\n",
    "\n",
    "# unigram case \n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_finegrain, y_train_finegrain, X_test_finegrain, y_test_finegrain,\n",
    "                                                                                   vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.3909502262443439 time: 0.8842999935150146\n",
      "accuracy with 5000 features: 0.4 time: 0.9687440395355225\n",
      "accuracy with 7500 features: 0.40542986425339367 time: 1.0097119808197021\n",
      "accuracy with 10000 features: 0.4149321266968326 time: 0.8717038631439209\n",
      "accuracy with 12500 features: 0.4108597285067873 time: 0.8585162162780762\n",
      "accuracy with 15000 features: 0.4108597285067873 time: 0.8899269104003906\n",
      "accuracy with 17500 features: 0.40588235294117647 time: 0.9269199371337891\n",
      "accuracy with 20000 features: 0.4076923076923077 time: 0.8048450946807861\n",
      "accuracy with 22500 features: 0.4072398190045249 time: 0.805927038192749\n",
      "accuracy with 25000 features: 0.4108597285067873 time: 0.812014102935791\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.3832579185520362 time: 0.786909818649292\n",
      "accuracy with 5000 features: 0.38868778280542987 time: 0.807304859161377\n",
      "accuracy with 7500 features: 0.39638009049773754 time: 0.8173198699951172\n",
      "accuracy with 10000 features: 0.3909502262443439 time: 0.7953059673309326\n",
      "accuracy with 12500 features: 0.3927601809954751 time: 0.8020708560943604\n",
      "accuracy with 15000 features: 0.3914027149321267 time: 0.8026909828186035\n",
      "accuracy with 17500 features: 0.3909502262443439 time: 0.8118348121643066\n",
      "accuracy with 20000 features: 0.3923076923076923 time: 0.818558931350708\n",
      "accuracy with 22500 features: 0.3909502262443439 time: 0.9109091758728027\n",
      "accuracy with 25000 features: 0.3932126696832579 time: 0.8657500743865967\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.38823529411764707 time: 0.6807370185852051\n",
      "accuracy with 5000 features: 0.3927601809954751 time: 0.64432692527771\n",
      "accuracy with 7500 features: 0.3936651583710407 time: 0.6291511058807373\n",
      "accuracy with 10000 features: 0.39592760180995473 time: 0.7307572364807129\n",
      "accuracy with 12500 features: 0.39773755656108595 time: 0.6915740966796875\n",
      "accuracy with 15000 features: 0.3950226244343891 time: 0.596656084060669\n",
      "accuracy with 17500 features: 0.3927601809954751 time: 0.6001832485198975\n",
      "accuracy with 20000 features: 0.3900452488687783 time: 0.6995840072631836\n",
      "accuracy with 22500 features: 0.38687782805429866 time: 0.7343869209289551\n",
      "accuracy with 25000 features: 0.38597285067873305 time: 0.6863360404968262\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 2500 features: 0.38778280542986426 time: 0.6228899955749512\n",
      "accuracy with 5000 features: 0.3904977375565611 time: 0.6047489643096924\n",
      "accuracy with 7500 features: 0.39547511312217193 time: 0.6109981536865234\n",
      "accuracy with 10000 features: 0.38733031674208146 time: 0.707679033279419\n",
      "accuracy with 12500 features: 0.38552036199095024 time: 0.8041181564331055\n",
      "accuracy with 15000 features: 0.38235294117647056 time: 0.6093130111694336\n",
      "accuracy with 17500 features: 0.38099547511312215 time: 0.6075167655944824\n",
      "accuracy with 20000 features: 0.37963800904977374 time: 0.692054033279419\n",
      "accuracy with 22500 features: 0.37963800904977374 time: 0.6378209590911865\n",
      "accuracy with 25000 features: 0.38009049773755654 time: 0.6002769470214844\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram case \n",
    "# in here, we modify the number of features from the range 30000 to 130000, since \n",
    "# much more features will be created\n",
    "\n",
    "n_features = np.arange(2500,25001,2500)\n",
    "\n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_finegrain, y_train_finegrain, X_test_finegrain, y_test_finegrain,\n",
    "                                                                vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], ngram_range = (1, 2), classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 20000 features: 0.41402714932126694 time: 1.535559892654419\n",
      "accuracy with 25000 features: 0.4090497737556561 time: 1.5398168563842773\n",
      "accuracy with 30000 features: 0.4144796380090498 time: 1.611750841140747\n",
      "accuracy with 35000 features: 0.4081447963800905 time: 1.5501508712768555\n",
      "accuracy with 40000 features: 0.40588235294117647 time: 1.7565877437591553\n",
      "accuracy with 45000 features: 0.4004524886877828 time: 1.4938406944274902\n",
      "accuracy with 50000 features: 0.3990950226244344 time: 1.5747942924499512\n",
      "accuracy with 55000 features: 0.39773755656108595 time: 1.5381629467010498\n",
      "accuracy with 60000 features: 0.39592760180995473 time: 1.8488011360168457\n",
      "accuracy with 65000 features: 0.39773755656108595 time: 1.469877004623413\n",
      "accuracy with 70000 features: 0.40180995475113124 time: 1.617008924484253\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 20000 features: 0.3918552036199095 time: 1.7208693027496338\n",
      "accuracy with 25000 features: 0.38687782805429866 time: 1.721215009689331\n",
      "accuracy with 30000 features: 0.3904977375565611 time: 1.665626049041748\n",
      "accuracy with 35000 features: 0.3900452488687783 time: 1.6058907508850098\n",
      "accuracy with 40000 features: 0.3914027149321267 time: 1.5818889141082764\n",
      "accuracy with 45000 features: 0.3895927601809955 time: 1.489030122756958\n",
      "accuracy with 50000 features: 0.3891402714932127 time: 1.9243659973144531\n",
      "accuracy with 55000 features: 0.38778280542986426 time: 1.996812105178833\n",
      "accuracy with 60000 features: 0.38823529411764707 time: 2.0602309703826904\n",
      "accuracy with 65000 features: 0.38687782805429866 time: 1.757209062576294\n",
      "accuracy with 70000 features: 0.38733031674208146 time: 2.1177048683166504\n",
      "\n",
      "\n",
      "==============================NOT INCLUDING STOP WORDS============================\n",
      "\t=================COUNTVECTORIZER===============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 20000 features: 0.3932126696832579 time: 1.8328211307525635\n",
      "accuracy with 25000 features: 0.38552036199095024 time: 1.3141028881072998\n",
      "accuracy with 30000 features: 0.38552036199095024 time: 1.7166521549224854\n",
      "accuracy with 35000 features: 0.38823529411764707 time: 0.9367568492889404\n",
      "accuracy with 40000 features: 0.38868778280542987 time: 0.9692611694335938\n",
      "accuracy with 45000 features: 0.3904977375565611 time: 0.9246377944946289\n",
      "accuracy with 50000 features: 0.3918552036199095 time: 0.929584264755249\n",
      "accuracy with 55000 features: 0.3927601809954751 time: 0.9708127975463867\n",
      "accuracy with 60000 features: 0.3918552036199095 time: 0.9426629543304443\n",
      "accuracy with 65000 features: 0.3914027149321267 time: 0.9532887935638428\n",
      "accuracy with 70000 features: 0.3914027149321267 time: 1.1364002227783203\n",
      "\n",
      "\n",
      "\t=================TFIDF=============\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "accuracy with 20000 features: 0.38009049773755654 time: 0.9852001667022705\n",
      "accuracy with 25000 features: 0.37873303167420813 time: 0.9415111541748047\n",
      "accuracy with 30000 features: 0.3778280542986425 time: 0.9332070350646973\n",
      "accuracy with 35000 features: 0.37963800904977374 time: 0.9107449054718018\n",
      "accuracy with 40000 features: 0.38054298642533935 time: 0.9291179180145264\n",
      "accuracy with 45000 features: 0.38280542986425337 time: 0.9363069534301758\n",
      "accuracy with 50000 features: 0.38144796380090495 time: 0.9682919979095459\n",
      "accuracy with 55000 features: 0.38144796380090495 time: 0.9485211372375488\n",
      "accuracy with 60000 features: 0.38054298642533935 time: 1.0032289028167725\n",
      "accuracy with 65000 features: 0.38009049773755654 time: 0.9669108390808105\n",
      "accuracy with 70000 features: 0.38009049773755654 time: 1.042341947555542\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram + trigram case\n",
    "# We furthur extent the number of features to the range of 120000\n",
    "\n",
    "n_features = np.arange(20000,70001,5000)\n",
    "\n",
    "for i in range(len(stop_word_extractor)):\n",
    "    if i == 0:\n",
    "        print(\"==============================INCLUDING STOP WORDS============================\")\n",
    "    else:\n",
    "        print(\"==============================NOT INCLUDING STOP WORDS============================\")\n",
    "    for j in range(len(vectorizer)):\n",
    "        if j == 0:\n",
    "            print(\"\\t=================COUNTVECTORIZER===============\")\n",
    "        else:\n",
    "            print(\"\\t=================TFIDF=============\")\n",
    "        feature_result_unigram = nfeature_accuracy_checker(X_train_finegrain, y_train_finegrain, X_test_finegrain, y_test_finegrain,\n",
    "                                                                vectorizer=vectorizer[j], n_features=n_features, stop_words=stop_word_extractor[i], ngram_range = (1, 3), classifier=mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, the unigram case with CountVectorizer 5000 features has the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnbGV = MultinomialNB()\n",
    "\n",
    "\n",
    "cvecGV = CountVectorizer(ngram_range = (1, 1), max_features = 5000)\n",
    "X_cvec = cvecGV.fit_transform(X_train_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_fg(alphas):\n",
    "    best_alpha = None\n",
    "    best_result = 0\n",
    "    \n",
    "    for a in alphas:\n",
    "        mnb_model = MultinomialNB(alpha = a)\n",
    "        X_final = cvecGV.transform(X_test)\n",
    "        t0 = time()\n",
    "        mnb_model.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = mnb_model.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"accuracy for alpha =\", a, \":\", result, \"time:\", train_test_time)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_alpha = a\n",
    "        \n",
    "    print(\"The best result is\",best_result,\"with alpha of\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for alpha = 0.6 : 0.4176470588235294 time: 0.012890100479125977\n",
      "accuracy for alpha = 0.7 : 0.41990950226244345 time: 0.006804943084716797\n",
      "accuracy for alpha = 0.8 : 0.4167420814479638 time: 0.006578922271728516\n",
      "accuracy for alpha = 0.9 : 0.4153846153846154 time: 0.009605884552001953\n",
      "accuracy for alpha = 1.0 : 0.4176470588235294 time: 0.007088899612426758\n",
      "The best result is 0.41990950226244345 with alpha of 0.7\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "hyper_parameter_tuning_fg(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, the best alpha is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 0.5)\n",
    "X_final = cvecGV.transform(X_test_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_sample_optimization_fg(samples):\n",
    "    best_sample = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for s in samples:\n",
    "        bg = BaggingClassifier(mnb, max_samples = s, max_features = 0.5, n_estimators = 200)\n",
    "        t0 = time()\n",
    "        bg.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {} with the time of {}\".format(s, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_sample = s\n",
    "    print(\"The best result is {} ratio of sample with accuracy of {}\".format(best_sample, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.4 ratio of sample is 0.4095022624434389 with the time of 1.7723388671875\n",
      "Bagging Classifier with 0.5 ratio of sample is 0.4171945701357466 with the time of 1.7434899806976318\n",
      "Bagging Classifier with 0.6 ratio of sample is 0.41990950226244345 with the time of 1.5583748817443848\n",
      "Bagging Classifier with 0.7 ratio of sample is 0.41809954751131223 with the time of 1.516690969467163\n",
      "Bagging Classifier with 0.8 ratio of sample is 0.41357466063348414 with the time of 1.6426348686218262\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.42036199095022625 with the time of 1.5503368377685547\n",
      "Bagging Classifier with 1.0 ratio of sample is 0.4104072398190045 with the time of 1.5544908046722412\n",
      "The best result is 0.9 ratio of sample with accuracy of 0.42036199095022625\n"
     ]
    }
   ],
   "source": [
    "features = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "bagging_max_sample_optimization_fg(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best sample ratio was 0.9\n",
    "bestSampleRatioFG = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_feature_optimization_fg(features):\n",
    "    best_feature = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for f in features:\n",
    "        bg = BaggingClassifier(mnb, max_samples = bestSampleRatioFG, max_features = f, n_estimators = 200)\n",
    "        t0 = time()\n",
    "        bg.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {} with the time of {}\".format(f, result, train_Test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_feature = f\n",
    "    print(\"The best result is {} ratio of sample with accuracy of {}\".format(best_feature, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.6 ratio of sample is 0.41402714932126694 with the time of 1.4933462142944336\n",
      "Bagging Classifier with 0.7 ratio of sample is 0.4095022624434389 with the time of 1.4521987438201904\n",
      "Bagging Classifier with 0.8 ratio of sample is 0.4176470588235294 with the time of 1.477698802947998\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.40588235294117647 with the time of 1.486508846282959\n",
      "Bagging Classifier with 1.0 ratio of sample is 0.4158371040723982 with the time of 1.4859158992767334\n",
      "The best result is 0.8 ratio of sample with accuracy of 0.4176470588235294\n"
     ]
    }
   ],
   "source": [
    "samples = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "bagging_max_sample_optimization_fg(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best feature ratio so far is 0.8\n",
    "bestFeatureRatioFG = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_max_estimator_optimization(estimators):\n",
    "    best_estim = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for e in estimators:\n",
    "        bg = BaggingClassifier(mnb, max_samples = bestSampleRatioFG, max_features = bestFeatureRatioFG, n_estimators = e)\n",
    "        t0 = time()\n",
    "        bg.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = bg.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {} with the time of {}\".format(e, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_estim = e\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_estim, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 400 ratio of sample is 0.4153846153846154 with the time of 3.294645071029663\n",
      "Bagging Classifier with 600 ratio of sample is 0.41855203619909503 with the time of 4.982913017272949\n",
      "Bagging Classifier with 800 ratio of sample is 0.4149321266968326 with the time of 6.9745306968688965\n",
      "Bagging Classifier with 1000 ratio of sample is 0.4167420814479638 with the time of 8.740811109542847\n",
      "The best result is 600 estimators with accuracy of 0.41855203619909503\n"
     ]
    }
   ],
   "source": [
    "estimators = [400, 600, 800, 1000]\n",
    "bagging_max_estimator_optimization(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================AdaBoost Classifier=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 0.9)\n",
    "X_final = cvecGV.transform(X_test_finegrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_max_estimator_optimization_fg(estimators):\n",
    "    best_estim = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for e in estimators:\n",
    "        adb = AdaBoostClassifier(mnb, n_estimators = e, learning_rate = 0.5)\n",
    "        t0  = time()\n",
    "        adb.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = adb.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Boosting Classifier with {} ratio of sample is {} with the time of {}\".format(e, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_estim = e\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_estim, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Classifier with 600 ratio of sample is 0.38144796380090495 with the time of 6.964943885803223\n",
      "Boosting Classifier with 800 ratio of sample is 0.38823529411764707 with the time of 10.121622085571289\n",
      "Boosting Classifier with 1000 ratio of sample is 0.39773755656108595 with the time of 12.930877923965454\n",
      "Boosting Classifier with 1200 ratio of sample is 0.39547511312217193 with the time of 14.2639639377594\n",
      "Boosting Classifier with 1400 ratio of sample is 0.3995475113122172 with the time of 15.808995962142944\n",
      "The best result is 1400 estimators with accuracy of 0.3995475113122172\n"
     ]
    }
   ],
   "source": [
    "estimators = [600, 800, 1000, 1200, 1400]\n",
    "boosting_max_estimator_optimization_fg(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestEstimators = 1400\n",
    "\n",
    "def boosting_best_lr_optimization_fg(learningrates):\n",
    "    best_lr = 0\n",
    "    best_result = 0\n",
    "    \n",
    "    for lr in learningrates:\n",
    "        adb = AdaBoostClassifier(mnb, n_estimators = bestEstimators, learning_rate = lr)\n",
    "        t0 = time()\n",
    "        adb.fit(X_cvec, y_train_finegrain)\n",
    "        y_pred = adb.predict(X_final)\n",
    "        train_test_time = time() - t0\n",
    "        result = accuracy_score(y_pred, y_test_finegrain)\n",
    "        print(\"Bagging Classifier with {} ratio of sample is {} with the time of {}\".format(lr, result, train_test_time))\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_lr= lr\n",
    "    print(\"The best result is {} estimators with accuracy of {}\".format(best_lr, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with 0.8 ratio of sample is 0.40180995475113124 with the time of 18.995368003845215\n",
      "Bagging Classifier with 0.9 ratio of sample is 0.3990950226244344 with the time of 18.766314029693604\n",
      "Bagging Classifier with 1.0 ratio of sample is 0.39592760180995473 with the time of 17.18435502052307\n",
      "Bagging Classifier with 1.1 ratio of sample is 0.39592760180995473 with the time of 17.47844386100769\n",
      "Bagging Classifier with 1.2 ratio of sample is 0.39592760180995473 with the time of 17.57903003692627\n",
      "The best result is 0.8 estimators with accuracy of 0.40180995475113124\n"
     ]
    }
   ],
   "source": [
    "learningrates = [0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "boosting_best_lr_optimization_fg(learningrates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
